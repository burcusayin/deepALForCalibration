{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ALforCalibration_MLP_mclass.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "en5IRda4XIVp",
        "outputId": "3dd5870c-af64-4821-b54c-98c4ffbe2c91"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install sklearn\n",
        "!pip install netcal"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: netcal in /usr/local/lib/python3.7/dist-packages (1.1.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from netcal) (0.22.2.post1)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.7/dist-packages (from netcal) (1.8.1+cu101)\n",
            "Requirement already satisfied: matplotlib>=3.1 in /usr/local/lib/python3.7/dist-packages (from netcal) (3.2.2)\n",
            "Requirement already satisfied: scipy>=1.3 in /usr/local/lib/python3.7/dist-packages (from netcal) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from netcal) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from netcal) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->netcal) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.1->netcal) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1->netcal) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1->netcal) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1->netcal) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1->netcal) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.1->netcal) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwU1cqCKXmSi"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "from random import shuffle\n",
        "\n",
        "from netcal.metrics import ECE\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import brier_score_loss\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH4SinJeXRWo"
      },
      "source": [
        "# setup random seed\n",
        "seed = 2020\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# define the paths to your data\n",
        "data_folder = 'drive/My Drive/Colab Notebooks/deepALForCalibration/datasets/mclass/drug_relation/' #specify the path to the folder where you keep your datasets\n",
        "dataToTrain = '8_train_indexed_drug_relation_mclass.csv'              # file name for your training data\n",
        "dataToVal = '8_val_indexed_drug_relation_mclass.csv'                  # file name for your validation data\n",
        "dataToTest = '8_test_indexed_drug_relation_mclass.csv'                # file name for your test data\n",
        "res_path = 'drive/My Drive/Colab Notebooks/deepALForCalibration/res/' # specify the path to keep results\n",
        "logfile_name = \"_8-drugRelation-MLP3.csv\"        # specify the name of the result file\n",
        "\n",
        "# columns of the csv file used in the experiments: text/content for each item, gold labels for each item, confidence scores for each class, ID of each item \n",
        "# specify the column names of your data\n",
        "iID = 'itemID'                # give each item an ID, it will be used during active learning\n",
        "goldLabel = 'crowd_label'     # define the name of column where you keep the gold labels of your data\n",
        "txt = 'text'                  # define the name of column where you keep the items \n",
        "testGoldLabel = 'gold_label'  # define the name of column where you keep the gold labels of your test data\n",
        "\n",
        "# specify the active learning strategy you want to use\n",
        "#al_strategy = 'diversity' \n",
        "#resDiversity = 'drive/My Drive/Colab Notebooks/deepALForCalibration/res/diversityRankings_MLP_3x100_D3_resampledByOneSideSelection.csv'\n",
        "#al_strategy = 'uncertainty'\n",
        "al_strategy = 'random'\n",
        "\n",
        "logfile_name = al_strategy + logfile_name\n",
        "\n",
        "# PARAMETERS\n",
        "num_labels = 2                                                        # number of classes in your data\n",
        "mClass = [0, 1]                                                       # define all of possible classes\n",
        "minimum_training_items = 66                                           # minimum number of training items before we first train a model\n",
        "alBatchNum = 10                                                       # define the total number of batches in active learning pipeline\n",
        "alBatchSize = 180                                                     # define the size of one batch in active learning pipeline\n",
        "maxTfIdfFeat = 1024                                                   # define the maximum number of features for tfidf \n",
        "\n",
        "#MLP for Dataset 1\n",
        "#model = MLPClassifier(hidden_layer_sizes=(100, 100, 100, 100, 100), max_iter=500, alpha=0.001, activation = 'tanh', solver='sgd')\n",
        "#MLP for Dataset 2\n",
        "#model = MLPClassifier(hidden_layer_sizes=(100, 100, 100, 100, 100), max_iter=500, alpha=0.05, activation = 'tanh', solver='sgd')\n",
        "#MLp for Dataset 3\n",
        "#model = MLPClassifier(hidden_layer_sizes=(100, 100, 100, 100), max_iter=500, alpha=0.05, activation = 'relu', solver='adam')  # define the classification model you want to use\n",
        "#MLP for Dataset 4\n",
        "#model = MLPClassifier(hidden_layer_sizes=(100, 100, 100, 100, 100), max_iter=500, alpha=0.001, activation = 'tanh', solver='sgd')\n",
        "#MLP for Dataset 8\n",
        "model = MLPClassifier(hidden_layer_sizes=(100, 100, 100, 100, 100), max_iter=500, alpha=0.05, activation = 'relu', solver='adam')\n",
        "\n",
        "poolDataEmb_train = np.array([])\n",
        "poolDataEmb_val = np.array([])\n",
        "poolDataEmb_test = np.array([])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9LM7N6UScSg"
      },
      "source": [
        "# create log file\n",
        "res_path += logfile_name\n",
        "if len(mClass) == 2:\n",
        "    with open(res_path, 'w') as f:\n",
        "        c = 'alBatch, sampledIndices, pre_train, rec_train, f01_train, f1_train, f10_train, ece_train, brier_train, pre_val, rec_val, f01_val, f1_val, f10_val, ece_val, brier_val, pre_test, rec_test, f01_test, f1_test, f10_test, ece_test, brier_test'\n",
        "        f.write(c + '\\n')\n",
        "else:\n",
        "    with open(res_path, 'w') as f:\n",
        "        c = 'alBatch, sampledIndices, pre_train, rec_train, f01_train, f1_train, f10_train, ece_train, pre_val, rec_val, f01_val, f1_val, f10_val, ece_val, pre_test, rec_test, f01_test, f1_test, f10_test, ece_test'\n",
        "        f.write(c + '\\n')\n",
        "\n",
        "# specify data directories\n",
        "unlabeled_data_dir = data_folder + dataToTrain\n",
        "validation_data_dir = data_folder + dataToVal\n",
        "test_data_dir = data_folder + dataToTest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVYyaN0_Z5Fd"
      },
      "source": [
        "class DiversitySampling():\n",
        "\n",
        "    def __init__(self, verbose):\n",
        "        self.verbose = verbose\n",
        "    \n",
        "    def get_validation_rankings(self, model, validation_data, val_emb):\n",
        "        \"\"\"Get model outliers from unlabeled data \n",
        "    \n",
        "        Keyword arguments:\n",
        "            model -- current Machine Learning model for this task\n",
        "            unlabeled_data -- data that does not yet have a label\n",
        "            validation_data -- held out data drawn from the same distribution as the training data\n",
        "            number -- number of items to sample\n",
        "            limit -- sample from only this many items for faster sampling (-1 = no limit)\n",
        "    \n",
        "        An outlier is defined as \n",
        "        unlabeled_data with the lowest average from rank order of logits\n",
        "        where rank order is defined by validation data inference \n",
        "    \n",
        "        \"\"\"\n",
        "                \n",
        "        validation_rankings = [] # 2D array, every neuron by ordered list of output on validation data per neuron    \n",
        "    \n",
        "        # Get per-neuron scores from validation data\n",
        "        if self.verbose:\n",
        "            print(\"Getting neuron activation scores from validation data\")\n",
        "\n",
        "        pred = model.predict_proba(val_emb) \n",
        "\n",
        "        v = 0\n",
        "        for neuron_outputs in pred:\n",
        "            # initialize array if we haven't yet\n",
        "            if len(validation_rankings) == 0:\n",
        "                for output in list(neuron_outputs):\n",
        "                    validation_rankings.append([0.0] * len(validation_data))\n",
        "\n",
        "            n=0\n",
        "            for output in list(neuron_outputs):\n",
        "                validation_rankings[n][v] = output\n",
        "                n += 1\n",
        "            v +=1\n",
        "            \n",
        "        \n",
        "        # Rank-order the validation scores \n",
        "        v=0\n",
        "        for validation in validation_rankings:\n",
        "            validation.sort() \n",
        "            validation_rankings[v] = validation\n",
        "            v += 1\n",
        "          \n",
        "        return validation_rankings \n",
        "    \n",
        "    def get_rank(self, value, rankings):\n",
        "        \"\"\" get the rank of the value in an ordered array as a percentage \n",
        "    \n",
        "        Keyword arguments:\n",
        "            value -- the value for which we want to return the ranked value\n",
        "            rankings -- the ordered array in which to determine the value's ranking\n",
        "        \n",
        "        returns linear distance between the indexes where value occurs, in the\n",
        "        case that there is not an exact match with the ranked values    \n",
        "        \"\"\"\n",
        "        \n",
        "        index = 0 # default: ranking = 0\n",
        "        \n",
        "        for ranked_number in rankings:\n",
        "            if value < ranked_number:\n",
        "                break #NB: this O(N) loop could be optimized to O(log(N))\n",
        "            index += 1        \n",
        "        \n",
        "        if(index >= len(rankings)):\n",
        "            index = len(rankings) # maximum: ranking = 1\n",
        "            \n",
        "        elif(index > 0):\n",
        "            # get linear interpolation between the two closest indexes \n",
        "            \n",
        "            diff = rankings[index] - rankings[index - 1]\n",
        "            perc = value - rankings[index - 1]\n",
        "            linear = perc / diff\n",
        "            index = float(index - 1) + linear\n",
        "        \n",
        "        absolute_ranking = index / len(rankings)\n",
        "    \n",
        "        return(absolute_ranking)\n",
        "    \n",
        "    def get_model_outliers(self, dataPool, model, unlabeled_data, unl_emb, validation_data, val_emb, number):\n",
        "        \"\"\"Get model outliers from unlabeled data \n",
        "    \n",
        "        Keyword arguments:\n",
        "            model -- current Machine Learning model for this task\n",
        "            unlabeled_data -- data that does not yet have a label\n",
        "            validation_data -- held out data drawn from the same distribution as the training data\n",
        "            number -- number of items to sample\n",
        "            limit -- sample from only this many items for faster sampling (-1 = no limit)\n",
        "    \n",
        "        An outlier is defined as \n",
        "        unlabeled_data with the lowest average from rank order of logits\n",
        "        where rank order is defined by validation data inference \n",
        "    \n",
        "        \"\"\"\n",
        "    \n",
        "        # Get per-neuron scores from validation data\n",
        "        validation_rankings = self.get_validation_rankings(model, validation_data, val_emb)\n",
        "\n",
        "        # Iterate over unlabeled items\n",
        "        if self.verbose:\n",
        "            print(\"Getting rankings for unlabeled data\")\n",
        "    \n",
        "        outliers = []\n",
        "        pred = model.predict_proba(unl_emb) \n",
        "\n",
        "        itID = 0\n",
        "        for neuron_outputs in pred:\n",
        "            n=0\n",
        "            ranks = []\n",
        "            for output in neuron_outputs:\n",
        "                rank = self.get_rank(output, validation_rankings[n])\n",
        "                ranks.append(rank)\n",
        "                n += 1 \n",
        "            avgRank = 1 - (sum(ranks) / len(neuron_outputs)) # average rank\n",
        "            currentRow = unlabeled_data.iloc[[itID]].reset_index(drop=True)\n",
        "            rowIndex = currentRow.itemID.item()\n",
        "            row = dataPool.loc[dataPool[iID] == rowIndex]\n",
        "            row['avgRank'] = avgRank\n",
        "            outliers.append(row.values.flatten().tolist()) \n",
        "            itID += 1\n",
        "        outliers.sort(reverse=True, key=lambda x: x[-1])       \n",
        "        return outliers[:number:]       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-pASfloTKzi"
      },
      "source": [
        "def random_sampling(unknownIndices, nQuery):\n",
        "    '''Randomly samples the points'''\n",
        "    query_idx = random.sample(range(len(unknownIndices)), nQuery)\n",
        "    selectedIndex = unknownIndices[query_idx]\n",
        "    return selectedIndex\n",
        "\n",
        "#uncertainty sampling yaz        \n",
        "def uncertainty_sampling(dataPool, model, unl_emb, number):\n",
        "    '''Points are sampled according to uncertainty sampling criterion'''\n",
        "\n",
        "    pred = model.predict_proba(unl_emb)\n",
        "    uncertainty_scores = 1 - pred.max(axis=1)\n",
        "    score_indices = np.argsort(uncertainty_scores)\n",
        "    return score_indices[-number:]   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4EwXlvgazxW"
      },
      "source": [
        "## Feature Preparation\n",
        "def prepare_features(X_train, min_df=2, max_features=None, ngram_range=(1, 3)):\n",
        "    # compute tfidf features\n",
        "    tfidf = TfidfVectorizer(min_df=min_df, max_features=max_features,\n",
        "                strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n",
        "                ngram_range=ngram_range, use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
        "                stop_words=None, lowercase=False)\n",
        "\n",
        "    X_train_tfidf = tfidf.fit_transform(X_train).toarray()\n",
        "    return X_train_tfidf\n",
        "\n",
        "class Data():\n",
        "    \n",
        "    def __init__(self, filename):\n",
        "        \n",
        "        # each dataset will have a pool of data, together with their IDs and gold labels \n",
        "        self.poolData = np.array([])\n",
        "        self.poolGoldLabels = np.array([])\n",
        "        \n",
        "        dt = pd.read_csv(filename)\n",
        "        dt = dt.dropna()\n",
        "        indices = dt[iID].values\n",
        "        y = dt[goldLabel].values\n",
        "        X = prepare_features(dt[txt].tolist(), min_df= 0, max_features = maxTfIdfFeat, ngram_range = (1, 3))\n",
        "        self.data = dt\n",
        "        self.poolDataEmb = X\n",
        "        self.poolGoldLabels = y\n",
        "        self.poolDataIndices = indices\n",
        "        \n",
        "    def setStartState(self, nStart):\n",
        "        ''' This functions initialises fields indicesKnown and indicesUnknown which contain the datapoints having final labels(known) and still explorable(unknown) ones.\n",
        "        Input:\n",
        "        nStart -- number of labelled datapoints (size of indicesKnown)\n",
        "        '''\n",
        "        self.nStart = nStart\n",
        "        self.indicesKnown = np.array([])\n",
        "        self.indicesUnknown = np.array([])\n",
        "        \n",
        "        # get predefined points so that all classes are represented and initial classifier could be trained.\n",
        "\n",
        "        for cls in mClass:\n",
        "            indices = np.array(np.where(self.poolGoldLabels == cls)).tolist()[0]\n",
        "            sampledIndices = random.sample(indices, nStart // len(mClass))\n",
        "            dataIndices = np.array(self.poolDataIndices)\n",
        "            if self.indicesKnown.size == 0:\n",
        "                self.indicesKnown = dataIndices[sampledIndices]\n",
        "            else:\n",
        "                self.indicesKnown = np.concatenate(([self.indicesKnown, dataIndices[sampledIndices]])); \n",
        "        for i in self.poolDataIndices:\n",
        "            if i not in self.indicesKnown:\n",
        "                if self.indicesUnknown.size == 0:\n",
        "                    self.indicesUnknown = np.array([i])\n",
        "                else:\n",
        "                    self.indicesUnknown = np.concatenate(([self.indicesUnknown, np.array([i])]));\n",
        "\n",
        "# function to calculate the ECE score\n",
        "def ece_score(y_true, y_prob, n_bins=10):\n",
        "    ece = ECE(n_bins)\n",
        "    ece_val = ece.measure(y_prob, y_true)\n",
        "\n",
        "    return ece_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieL71IeGa1VT",
        "outputId": "606717bf-2070-4d89-fbcc-6cab9a4f2959"
      },
      "source": [
        "#load datasets\n",
        "pool = Data(unlabeled_data_dir)\n",
        "pool.setStartState(minimum_training_items)\n",
        "poolData = pool.data\n",
        "poolDataIndices = pool.poolDataIndices\n",
        "\n",
        "validation = Data(validation_data_dir)\n",
        "validation_data = validation.data\n",
        "test = Data(test_data_dir)\n",
        "test_data = test.data\n",
        "\n",
        "poolDataEmb_train = pool.poolDataEmb\n",
        "poolDataEmb_val = validation.poolDataEmb\n",
        "poolDataEmb_test = test.poolDataEmb\n",
        "\n",
        "training_data = poolData.loc[poolData[iID].isin(pool.indicesKnown)].reset_index(drop=True)\n",
        "train_data_idx = poolData.index[poolData[iID].isin(pool.indicesKnown)].tolist()\n",
        "train_data = poolDataEmb_train[train_data_idx]\n",
        "train_labels = np.array(training_data[goldLabel].tolist())\n",
        "\n",
        "model.fit(train_data, train_labels) \n",
        "    \n",
        "#Start active learning\n",
        "sampleIds = []\n",
        "samplingRanks = []\n",
        "\n",
        "for alBatch in range(alBatchNum):\n",
        "\n",
        "    unlabeled_data = poolData.loc[poolData[iID].isin(pool.indicesUnknown)].reset_index(drop=True)\n",
        "    unlabeled_data_idx = poolData.index[poolData[iID].isin(pool.indicesUnknown)].tolist()\n",
        "    unl_dataEmb = poolDataEmb_train[unlabeled_data_idx]\n",
        "\n",
        "    sampledIndices = []\n",
        "    if al_strategy == 'diversity':\n",
        "        strategy = DiversitySampling(True)\n",
        "        sampledItems = strategy.get_model_outliers(poolData, model, unlabeled_data, unl_dataEmb, validation_data, poolDataEmb_val, number=alBatchSize) \n",
        "        \n",
        "        for outlier in sampledItems:\n",
        "            samplingRanks.append(outlier[-1])\n",
        "            sampleIds.append(outlier[-2])\n",
        "            sampledIndices.append(outlier[-2])\n",
        "   \n",
        "    elif al_strategy == 'random':\n",
        "        sampledIndices = random_sampling(pool.indicesUnknown, alBatchSize)\n",
        "        for i in sampledIndices: sampleIds.append(i)\n",
        "    elif al_strategy == 'uncertainty':\n",
        "        sampledIndices = uncertainty_sampling(poolData, model, unl_dataEmb, alBatchSize)\n",
        "        for i in sampledIndices: sampleIds.append(i)\n",
        "    else:\n",
        "        # random sampling by default\n",
        "        sampledIndices = random_sampling(pool.indicesUnknown, alBatchSize)\n",
        "        for i in sampledIndices: sampleIds.append(i)\n",
        "\n",
        "    sampledInd = np.array(sampledIndices)\n",
        "    pool.indicesKnown = np.concatenate(([pool.indicesKnown, np.array(sampledInd)]))\n",
        "\n",
        "    pool.indicesUnknown = np.array([])\n",
        "    for i in poolDataIndices:\n",
        "        if i not in pool.indicesKnown:\n",
        "            pool.indicesUnknown = np.concatenate(([pool.indicesUnknown, np.array([i])])); \n",
        "\n",
        "    training_data = poolData.loc[poolData[iID].isin(pool.indicesKnown)].reset_index(drop=True)\n",
        "    train_data_idx = poolData.index[poolData[iID].isin(pool.indicesKnown)].tolist()\n",
        "    train_data = poolDataEmb_train[train_data_idx]\n",
        "    train_labels = np.array(training_data[goldLabel].tolist())\n",
        "    #print(\"Start training.\")\n",
        "    model.fit(train_data, train_labels) \n",
        "\n",
        "    y_pred_train = model.predict(train_data)\n",
        "    logits_train = model.predict_proba(train_data)\n",
        "    probs_train = np.array(logits_train)\n",
        "\n",
        "\n",
        "    y_pred_val = model.predict(poolDataEmb_val)\n",
        "    logits_val = model.predict_proba(poolDataEmb_val)\n",
        "    probs_val = np.array(logits_val)\n",
        "    val_labels = np.array(validation_data[goldLabel].tolist())\n",
        "\n",
        "    y_pred_test = model.predict(poolDataEmb_test)\n",
        "    logits_test = model.predict_proba(poolDataEmb_test)\n",
        "    probs_test = np.array(logits_test)\n",
        "    test_labels = np.array(test_data[testGoldLabel].tolist())\n",
        "\n",
        "    # check if binary or multi class classification\n",
        "    num_classes = len(set(val_labels))\n",
        "    if num_classes == 2:\n",
        "        average = 'binary'\n",
        "    else:\n",
        "        average = 'macro'\n",
        "\n",
        "    sampledItems = ''.join(str(e)+' ' for e in sampledIndices)\n",
        "\n",
        "    pre_train, rec_train, f1_train, _ = precision_recall_fscore_support(train_labels, y_pred_train, average=average, beta=1)\n",
        "    ece_train = ece_score(train_labels, probs_train)\n",
        "    _, _, f01_train, _ = precision_recall_fscore_support(train_labels, y_pred_train, average=average, beta=0.1)\n",
        "    _, _, f10_train, _ = precision_recall_fscore_support(train_labels, y_pred_train, average=average, beta=10)\n",
        "\n",
        "\n",
        "    pre_val, rec_val, f1_val, _ = precision_recall_fscore_support(val_labels, y_pred_val, average=average, beta=1)\n",
        "    ece_val = ece_score(val_labels, probs_val)\n",
        "    _, _, f01_val, _ = precision_recall_fscore_support(val_labels, y_pred_val, average=average, beta=0.1)\n",
        "    _, _, f10_val, _ = precision_recall_fscore_support(val_labels, y_pred_val, average=average, beta=10)\n",
        "\n",
        "    pre_test, rec_test, f1_test, _ = precision_recall_fscore_support(test_labels, y_pred_test, average=average, beta=1)\n",
        "    ece_test = ece_score(test_labels, probs_test)\n",
        "    _, _, f01_test, _ = precision_recall_fscore_support(test_labels, y_pred_test, average=average, beta=0.1)\n",
        "    _, _, f10_test, _ = precision_recall_fscore_support(test_labels, y_pred_test, average=average, beta=10)\n",
        "\n",
        "    if average == 'binary':\n",
        "        brier_train = brier_score_loss(train_labels, probs_train[:,1])\n",
        "        brier_val = brier_score_loss(val_labels, probs_val[:,1])\n",
        "        brier_test = brier_score_loss(test_labels, probs_test[:,1])\n",
        "\n",
        "        print(\n",
        "            'Iteration: {}. F1: {:1.3f}, Precision: {:1.3f}, Recall: {:1.3f}'.\n",
        "             format(alBatch, f1_val, pre_val, rec_val))\n",
        "        # print to result file\n",
        "        with open(res_path, 'a') as f:\n",
        "            res_i = '{}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}\\n'.format(alBatch, sampledItems, pre_train, rec_train, f01_train, f1_train, f10_train, ece_train, brier_train, pre_val, rec_val, f01_val, f1_val, f10_val, ece_val, brier_val, pre_test, rec_test, f01_test, f1_test, f10_test, ece_test, brier_test)\n",
        "            f.write(res_i)\n",
        "    else:\n",
        "    \n",
        "        print(\n",
        "            'Iteration: {}. F1: {:1.3f}, Precision: {:1.3f}, Recall: {:1.3f}'.\n",
        "             format(alBatch, f1_val, pre_val, rec_val))\n",
        "        # print to result file\n",
        "        with open(res_path, 'a') as f:\n",
        "            res_i = '{}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}\\n'.format(alBatch, sampledItems, pre_train, rec_train, f01_train, f1_train, f10_train, ece_train, pre_val, rec_val, f01_val, f1_val, f10_val, ece_val, pre_test, rec_test, f01_test, f1_test, f10_test, ece_test)\n",
        "            f.write(res_i)\n",
        "\n",
        "if al_strategy == 'diversity':\n",
        "    divRanking = pd.DataFrame(\n",
        "       {'sampleID': sampleIds,\n",
        "         'diversityRank': samplingRanks\n",
        "       })\n",
        "\n",
        "    divRanking.to_csv(resDiversity)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train_tfidf.shape (1866, 1024)\n",
            "prepared features:  [[0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.         0.         0.         ... 0.         0.28574674 0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
            "X_train_tfidf.shape (75, 1024)\n",
            "prepared features:  [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "X_train_tfidf.shape (72, 1024)\n",
            "prepared features:  [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "training_data:                                                  text  ...  itemID\n",
            "0   My wife and i were concerned about my rapid we...  ...      90\n",
            "1   It is probably wiser to do what is required to...  ...     132\n",
            "2   The paxil has greatly quieted the symptoms I h...  ...     133\n",
            "3   As others have said pain medications constipat...  ...     153\n",
            "4   Then again, I'm allergic to many meds : penici...  ...     159\n",
            "..                                                ...  ...     ...\n",
            "61  These disorders include:  cortisol hormone imb...  ...    1707\n",
            "62  Those seem to be the drugs that they use to co...  ...    1712\n",
            "63  Like Jeanne was saying that her father got dia...  ...    1768\n",
            "64  You can research and read the fullprescribing ...  ...    1788\n",
            "65  They can cause Major triglyceride and glucose ...  ...    1856\n",
            "\n",
            "[66 rows x 6 columns]\n",
            "train_data_idx:  [89, 131, 132, 152, 158, 210, 291, 295, 298, 329, 330, 331, 379, 433, 464, 469, 488, 501, 512, 526, 555, 641, 655, 677, 741, 806, 816, 817, 843, 845, 864, 871, 872, 1022, 1036, 1048, 1060, 1075, 1086, 1142, 1215, 1238, 1265, 1291, 1330, 1373, 1403, 1419, 1432, 1457, 1460, 1474, 1477, 1524, 1536, 1555, 1576, 1657, 1672, 1677, 1689, 1706, 1711, 1767, 1787, 1855]\n",
            "train_data[0]:  [0. 0. 0. ... 0. 0. 0.]\n",
            "train_labels:  [0 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0\n",
            " 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 1 1 1 1]\n",
            "Sampling 180 Model Outliers\n",
            "\n",
            "shape of unl_dataEmb :  (1800, 1024)\n",
            "average:  macro\n",
            "Iteration: 0. F1: 0.176, Precision: 0.143, Recall: 0.267\n",
            "Sampling 180 Model Outliers\n",
            "\n",
            "shape of unl_dataEmb :  (1620, 1024)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "average:  macro\n",
            "Iteration: 1. F1: 0.183, Precision: 0.149, Recall: 0.280\n",
            "Sampling 180 Model Outliers\n",
            "\n",
            "shape of unl_dataEmb :  (1440, 1024)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "average:  macro\n",
            "Iteration: 2. F1: 0.244, Precision: 0.210, Recall: 0.320\n",
            "Sampling 180 Model Outliers\n",
            "\n",
            "shape of unl_dataEmb :  (1260, 1024)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "average:  macro\n",
            "Iteration: 3. F1: 0.199, Precision: 0.170, Recall: 0.267\n",
            "Sampling 180 Model Outliers\n",
            "\n",
            "shape of unl_dataEmb :  (1080, 1024)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "average:  macro\n",
            "Iteration: 4. F1: 0.200, Precision: 0.173, Recall: 0.280\n",
            "Sampling 180 Model Outliers\n",
            "\n",
            "shape of unl_dataEmb :  (900, 1024)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "average:  macro\n",
            "Iteration: 5. F1: 0.224, Precision: 0.198, Recall: 0.307\n",
            "Sampling 180 Model Outliers\n",
            "\n",
            "shape of unl_dataEmb :  (720, 1024)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "average:  macro\n",
            "Iteration: 6. F1: 0.241, Precision: 0.520, Recall: 0.293\n",
            "Sampling 180 Model Outliers\n",
            "\n",
            "shape of unl_dataEmb :  (540, 1024)\n",
            "average:  macro\n",
            "Iteration: 7. F1: 0.258, Precision: 0.225, Recall: 0.333\n",
            "Sampling 180 Model Outliers\n",
            "\n",
            "shape of unl_dataEmb :  (360, 1024)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "average:  macro\n",
            "Iteration: 8. F1: 0.224, Precision: 0.200, Recall: 0.320\n",
            "Sampling 180 Model Outliers\n",
            "\n",
            "shape of unl_dataEmb :  (180, 1024)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "average:  macro\n",
            "Iteration: 9. F1: 0.232, Precision: 0.212, Recall: 0.320\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}