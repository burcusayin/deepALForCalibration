{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "en5IRda4XIVp",
    "outputId": "3dd5870c-af64-4821-b54c-98c4ffbe2c91"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install sklearn\n",
    "# !pip install netcal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hwU1cqCKXmSi"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from random import shuffle\n",
    "\n",
    "from netcal.metrics import ECE\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KH4SinJeXRWo"
   },
   "outputs": [],
   "source": [
    "# setup random seed\n",
    "seed = 2020\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "# define the paths to your data\n",
    "#data_folder = 'drive/My Drive/Colab Notebooks/deepALForCalibration/datasets/binary/economic_news/'  \n",
    "#specify the path to the folder where you keep your datasets\n",
    "#dataToTrain = '4_train_indexed_economic_news_binary.csv'            # file name for your training data\n",
    "#dataToVal = '4_val_indexed_economic_news_binary.csv'                 # file name for your validation data\n",
    "#dataToTest = '4_test_indexed_economic_news_binary.csv'                # file name for your test data\n",
    "data_folder = '/Users/fabio.casati/Documents/dev/labdev/data/az/' #specify the path to the folder where you keep your datasets\n",
    "dataToTrain = 'df0L.csv'              # file name for your training data\n",
    "dataToVal = 'df0L.csv'                  # file name for your validation data\n",
    "dataToTest = 'df0L.csv'                # file name for your test data\n",
    "\n",
    "res_path = './' # specify the path to keep results\n",
    "logfile_name = \"azlog.csv\"        # specify the name of the result file\n",
    "\n",
    "# columns of the csv file used in the experiments: text/content for each item, gold labels for each item, confidence scores for each class, ID of each item \n",
    "# specify the column names of your data\n",
    "iID = 'sys_id'             # give each item an ID, it will be used during active learning\n",
    "goldLabel = 'assignment_group'  # define the name of column where you keep the gold labels of your data\n",
    "txt = 'short_description'               # define the name of column where you keep the items \n",
    "testGoldLabel = 'assignment_group'\n",
    "\n",
    "# specify the active learning strategy you want to use\n",
    "al_strategy = 'diversity' \n",
    "#resDiversity = 'drive/My Drive/Colab Notebooks/deepALForCalibration/res/diversityRankings_MLP_3x100_D3_resampledByOneSideSelection.csv'\n",
    "#al_strategy = 'uncertainty'\n",
    "#al_strategy = 'random'\n",
    "\n",
    "logfile_name = al_strategy + logfile_name\n",
    "\n",
    "# PARAMETERS\n",
    "num_labels = 2                                                        # number of classes in your data\n",
    "mClass = [0, 1]                                                       # define all of possible classes\n",
    "minimum_training_items = 66                                           # minimum number of training items before we first train a model\n",
    "alBatchNum = 10                                                       # define the total number of batches in active learning pipeline\n",
    "alBatchSize = 180                                                     # define the size of one batch in active learning pipeline\n",
    "maxTfIdfFeat = 1024                                                   # define the maximum number of features for tfidf \n",
    "\n",
    "#MLP for Dataset 1\n",
    "#model = MLPClassifier(hidden_layer_sizes=(100, 100, 100, 100, 100), max_iter=500, alpha=0.001, activation = 'tanh', solver='sgd')\n",
    "#MLP for Dataset 2\n",
    "#model = MLPClassifier(hidden_layer_sizes=(100, 100, 100, 100, 100), max_iter=500, alpha=0.05, activation = 'tanh', solver='sgd')\n",
    "#MLp for Dataset 3\n",
    "#model = MLPClassifier(hidden_layer_sizes=(100, 100, 100, 100), max_iter=500, alpha=0.05, activation = 'relu', solver='adam')  # define the classification model you want to use\n",
    "#MLP for Dataset 4\n",
    "#model = MLPClassifier(hidden_layer_sizes=(100, 100, 100, 100, 100), max_iter=500, alpha=0.001, activation = 'tanh', solver='sgd')\n",
    "#MLP for Dataset 8\n",
    "model = MLPClassifier(hidden_layer_sizes=(100, 100, 100, 100, 100), max_iter=500, alpha=0.05, activation = 'relu', solver='adam')\n",
    "\n",
    "poolDataEmb_train = np.array([])\n",
    "poolDataEmb_val = np.array([])\n",
    "poolDataEmb_test = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "G9LM7N6UScSg"
   },
   "outputs": [],
   "source": [
    "# create log file\n",
    "res_path += logfile_name\n",
    "if len(mClass) == 2:\n",
    "    with open(res_path, 'w') as f:\n",
    "        c = 'alBatch, sampledIndices, pre_train, rec_train, f01_train, f1_train, f10_train, ece_train, brier_train, pre_val, rec_val, f01_val, f1_val, f10_val, ece_val, brier_val, pre_test, rec_test, f01_test, f1_test, f10_test, ece_test, brier_test'\n",
    "        f.write(c + '\\n')\n",
    "else:\n",
    "    with open(res_path, 'w') as f:\n",
    "        c = 'alBatch, sampledIndices, pre_train, rec_train, f01_train, f1_train, f10_train, ece_train, pre_val, rec_val, f01_val, f1_val, f10_val, ece_val, pre_test, rec_test, f01_test, f1_test, f10_test, ece_test'\n",
    "        f.write(c + '\\n')\n",
    "\n",
    "# specify data directories\n",
    "unlabeled_data_dir = data_folder + dataToTrain\n",
    "validation_data_dir = data_folder + dataToVal\n",
    "test_data_dir = data_folder + dataToTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pVYyaN0_Z5Fd"
   },
   "outputs": [],
   "source": [
    "class DiversitySampling():\n",
    "\n",
    "    def __init__(self, verbose):\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def get_validation_rankings(self, model, validation_data, val_emb):\n",
    "        \"\"\"Get model outliers from unlabeled data \n",
    "    \n",
    "        Keyword arguments:\n",
    "            model -- current Machine Learning model for this task\n",
    "            unlabeled_data -- data that does not yet have a label\n",
    "            validation_data -- held out data drawn from the same distribution as the training data\n",
    "            number -- number of items to sample\n",
    "            limit -- sample from only this many items for faster sampling (-1 = no limit)\n",
    "    \n",
    "        An outlier is defined as \n",
    "        unlabeled_data with the lowest average from rank order of logits\n",
    "        where rank order is defined by validation data inference \n",
    "    \n",
    "        \"\"\"\n",
    "                \n",
    "        validation_rankings = [] # 2D array, every neuron by ordered list of output on validation data per neuron    \n",
    "    \n",
    "        # Get per-neuron scores from validation data\n",
    "        if self.verbose:\n",
    "            print(\"Getting neuron activation scores from validation data\")\n",
    "\n",
    "        pred = model.predict_proba(val_emb) \n",
    "\n",
    "        v = 0\n",
    "        for neuron_outputs in pred:\n",
    "            # initialize array if we haven't yet\n",
    "            if len(validation_rankings) == 0:\n",
    "                for output in list(neuron_outputs):\n",
    "                    validation_rankings.append([0.0] * len(validation_data))\n",
    "\n",
    "            n=0\n",
    "            for output in list(neuron_outputs):\n",
    "                validation_rankings[n][v] = output\n",
    "                n += 1\n",
    "            v +=1\n",
    "            \n",
    "        \n",
    "        # Rank-order the validation scores \n",
    "        v=0\n",
    "        for validation in validation_rankings:\n",
    "            validation.sort() \n",
    "            validation_rankings[v] = validation\n",
    "            v += 1\n",
    "          \n",
    "        return validation_rankings \n",
    "    \n",
    "    def get_rank(self, value, rankings):\n",
    "        \"\"\" get the rank of the value in an ordered array as a percentage \n",
    "    \n",
    "        Keyword arguments:\n",
    "            value -- the value for which we want to return the ranked value\n",
    "            rankings -- the ordered array in which to determine the value's ranking\n",
    "        \n",
    "        returns linear distance between the indexes where value occurs, in the\n",
    "        case that there is not an exact match with the ranked values    \n",
    "        \"\"\"\n",
    "        \n",
    "        index = 0 # default: ranking = 0\n",
    "        \n",
    "        for ranked_number in rankings:\n",
    "            if value < ranked_number:\n",
    "                break #NB: this O(N) loop could be optimized to O(log(N))\n",
    "            index += 1        \n",
    "        \n",
    "        if(index >= len(rankings)):\n",
    "            index = len(rankings) # maximum: ranking = 1\n",
    "            \n",
    "        elif(index > 0):\n",
    "            # get linear interpolation between the two closest indexes \n",
    "            \n",
    "            diff = rankings[index] - rankings[index - 1]\n",
    "            perc = value - rankings[index - 1]\n",
    "            linear = perc / diff\n",
    "            index = float(index - 1) + linear\n",
    "        \n",
    "        absolute_ranking = index / len(rankings)\n",
    "    \n",
    "        return(absolute_ranking)\n",
    "    \n",
    "    def get_model_outliers(self, dataPool, model, unlabeled_data, unl_emb, validation_data, val_emb, number):\n",
    "        \"\"\"Get model outliers from unlabeled data \n",
    "    \n",
    "        Keyword arguments:\n",
    "            model -- current Machine Learning model for this task\n",
    "            unlabeled_data -- data that does not yet have a label\n",
    "            validation_data -- held out data drawn from the same distribution as the training data\n",
    "            number -- number of items to sample\n",
    "            limit -- sample from only this many items for faster sampling (-1 = no limit)\n",
    "    \n",
    "        An outlier is defined as \n",
    "        unlabeled_data with the lowest average from rank order of logits\n",
    "        where rank order is defined by validation data inference \n",
    "    \n",
    "        \"\"\"\n",
    "    \n",
    "        # Get per-neuron scores from validation data\n",
    "        validation_rankings = self.get_validation_rankings(model, validation_data, val_emb)\n",
    "\n",
    "        # Iterate over unlabeled items\n",
    "        if self.verbose:\n",
    "            print(\"Getting rankings for unlabeled data\")\n",
    "    \n",
    "        outliers = []\n",
    "        pred = model.predict_proba(unl_emb) \n",
    "\n",
    "        itID = 0\n",
    "        for neuron_outputs in pred:\n",
    "            n=0\n",
    "            ranks = []\n",
    "            for output in neuron_outputs:\n",
    "                rank = self.get_rank(output, validation_rankings[n])\n",
    "                ranks.append(rank)\n",
    "                n += 1 \n",
    "            avgRank = 1 - (sum(ranks) / len(neuron_outputs)) # average rank\n",
    "            currentRow = unlabeled_data.iloc[[itID]].reset_index(drop=True)\n",
    "            rowIndex = currentRow.itemID.item()\n",
    "            row = dataPool.loc[dataPool[iID] == rowIndex]\n",
    "            row['avgRank'] = avgRank\n",
    "            outliers.append(row.values.flatten().tolist()) \n",
    "            itID += 1\n",
    "        outliers.sort(reverse=True, key=lambda x: x[-1])       \n",
    "        return outliers[:number:]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "g-pASfloTKzi"
   },
   "outputs": [],
   "source": [
    "def random_sampling(unknownIndices, nQuery):\n",
    "    '''Randomly samples the points'''\n",
    "    query_idx = random.sample(range(len(unknownIndices)), nQuery)\n",
    "    selectedIndex = unknownIndices[query_idx]\n",
    "    return selectedIndex\n",
    "\n",
    "#uncertainty sampling yaz        \n",
    "def uncertainty_sampling(dataPool, model, unl_emb, number):\n",
    "    '''Points are sampled according to uncertainty sampling criterion'''\n",
    "\n",
    "    pred = model.predict_proba(unl_emb)\n",
    "    uncertainty_scores = 1 - pred.max(axis=1)\n",
    "    score_indices = np.argsort(uncertainty_scores)\n",
    "    return score_indices[-number:]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_4EwXlvgazxW"
   },
   "outputs": [],
   "source": [
    "## Feature Preparation\n",
    "def prepare_features(X_train, min_df=2, max_features=None, ngram_range=(1, 3)):\n",
    "    # compute tfidf features\n",
    "    tfidf = TfidfVectorizer(min_df=min_df, max_features=max_features,\n",
    "                strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n",
    "                ngram_range=ngram_range, use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "                stop_words=None, lowercase=False)\n",
    "\n",
    "    X_train_tfidf = tfidf.fit_transform(X_train).toarray()\n",
    "    return X_train_tfidf\n",
    "\n",
    "class Data():\n",
    "    \n",
    "    def __init__(self, filename):\n",
    "        \n",
    "        # each dataset will have a pool of data, together with their IDs and gold labels \n",
    "        self.poolData = np.array([])\n",
    "        self.poolGoldLabels = np.array([])\n",
    "        \n",
    "        dt = pd.read_csv(filename)\n",
    "        dt = dt.dropna()\n",
    "        indices = dt[iID].values\n",
    "        y = dt[goldLabel].values\n",
    "        X = prepare_features(dt[txt].tolist(), min_df= 0, max_features = maxTfIdfFeat, ngram_range = (1, 3))\n",
    "        self.data = dt\n",
    "        self.poolDataEmb = X\n",
    "        self.poolGoldLabels = y\n",
    "        self.poolDataIndices = indices\n",
    "        \n",
    "    def setStartState(self, nStart):\n",
    "        ''' This functions initialises fields indicesKnown and indicesUnknown which contain the datapoints having final labels(known) and still explorable(unknown) ones.\n",
    "        Input:\n",
    "        nStart -- number of labelled datapoints (size of indicesKnown)\n",
    "        '''\n",
    "        self.nStart = nStart\n",
    "        self.indicesKnown = np.array([])\n",
    "        self.indicesUnknown = np.array([])\n",
    "        \n",
    "        # get predefined points so that all classes are represented and initial classifier could be trained.\n",
    "\n",
    "        for cls in mClass:\n",
    "            indices = np.array(np.where(self.poolGoldLabels == cls)).tolist()[0]\n",
    "            sampledIndices = random.sample(indices, nStart // len(mClass))\n",
    "            dataIndices = np.array(self.poolDataIndices)\n",
    "            if self.indicesKnown.size == 0:\n",
    "                self.indicesKnown = dataIndices[sampledIndices]\n",
    "            else:\n",
    "                self.indicesKnown = np.concatenate(([self.indicesKnown, dataIndices[sampledIndices]])); \n",
    "        for i in self.poolDataIndices:\n",
    "            if i not in self.indicesKnown:\n",
    "                if self.indicesUnknown.size == 0:\n",
    "                    self.indicesUnknown = np.array([i])\n",
    "                else:\n",
    "                    self.indicesUnknown = np.concatenate(([self.indicesUnknown, np.array([i])]));\n",
    "\n",
    "# function to calculate the ECE score\n",
    "def ece_score(y_true, y_prob, n_bins=10):\n",
    "    ece = ECE(n_bins)\n",
    "    ece_val = ece.measure(y_prob, y_true)\n",
    "\n",
    "    return ece_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ieL71IeGa1VT",
    "outputId": "606717bf-2070-4d89-fbcc-6cab9a4f2959"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabio.casati/.venvs/389/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3361: DtypeWarning: Columns (3,4,5,10,25,27,33,36,38,40,46,54,56,61,65,71,76,81,96,97,98,99,101,102,106,109,117,118,122,124,126,130,131,132,136,138,142,143,148,150,153,174,178,180,181,182,185,186,188,191,193,197,215,216,217,237,240,255,259,267,268,278,284,308,310,311,312,321,330,334,336,338,344,348,350,390,394,395,397,400,403,405,409) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a051c03f343d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#load datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munlabeled_data_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetStartState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminimum_training_items\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpoolData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpoolDataIndices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoolDataIndices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-fb612e8d3310>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miID\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgoldLabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaxTfIdfFeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoolDataEmb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-fb612e8d3310>\u001b[0m in \u001b[0;36mprepare_features\u001b[0;34m(X_train, min_df, max_features, ngram_range)\u001b[0m\n\u001b[1;32m      7\u001b[0m                 stop_words=None, lowercase=False)\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mX_train_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX_train_tfidf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/389/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1844\u001b[0m         \"\"\"\n\u001b[1;32m   1845\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1846\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1847\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/389/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1202\u001b[0;31m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[1;32m   1203\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/389/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1131\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[1;32m   1134\u001b[0m                                  \" contain stop words\")\n\u001b[1;32m   1135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "#load datasets\n",
    "pool = Data(unlabeled_data_dir)\n",
    "pool.setStartState(minimum_training_items)\n",
    "poolData = pool.data\n",
    "poolDataIndices = pool.poolDataIndices\n",
    "\n",
    "validation = Data(validation_data_dir)\n",
    "validation_data = validation.data\n",
    "test = Data(test_data_dir)\n",
    "test_data = test.data\n",
    "\n",
    "poolDataEmb_train = pool.poolDataEmb\n",
    "poolDataEmb_val = validation.poolDataEmb\n",
    "poolDataEmb_test = test.poolDataEmb\n",
    "\n",
    "training_data = poolData.loc[poolData[iID].isin(pool.indicesKnown)].reset_index(drop=True)\n",
    "train_data_idx = poolData.index[poolData[iID].isin(pool.indicesKnown)].tolist()\n",
    "train_data = poolDataEmb_train[train_data_idx]\n",
    "train_labels = np.array(training_data[goldLabel].tolist())\n",
    "\n",
    "model.fit(train_data, train_labels) \n",
    "    \n",
    "#Start active learning\n",
    "sampleIds = []\n",
    "samplingRanks = []\n",
    "\n",
    "for alBatch in range(alBatchNum):\n",
    "\n",
    "    unlabeled_data = poolData.loc[poolData[iID].isin(pool.indicesUnknown)].reset_index(drop=True)\n",
    "    unlabeled_data_idx = poolData.index[poolData[iID].isin(pool.indicesUnknown)].tolist()\n",
    "    unl_dataEmb = poolDataEmb_train[unlabeled_data_idx]\n",
    "\n",
    "    sampledIndices = []\n",
    "    if al_strategy == 'diversity':\n",
    "        strategy = DiversitySampling(True)\n",
    "        sampledItems = strategy.get_model_outliers(poolData, model, unlabeled_data, unl_dataEmb, validation_data, poolDataEmb_val, number=alBatchSize) \n",
    "        \n",
    "        for outlier in sampledItems:\n",
    "            samplingRanks.append(outlier[-1])\n",
    "            sampleIds.append(outlier[-2])\n",
    "            sampledIndices.append(outlier[-2])\n",
    "   \n",
    "    elif al_strategy == 'random':\n",
    "        sampledIndices = random_sampling(pool.indicesUnknown, alBatchSize)\n",
    "        for i in sampledIndices: sampleIds.append(i)\n",
    "    elif al_strategy == 'uncertainty':\n",
    "        sampledIndices = uncertainty_sampling(poolData, model, unl_dataEmb, alBatchSize)\n",
    "        for i in sampledIndices: sampleIds.append(i)\n",
    "    else:\n",
    "        # random sampling by default\n",
    "        sampledIndices = random_sampling(pool.indicesUnknown, alBatchSize)\n",
    "        for i in sampledIndices: sampleIds.append(i)\n",
    "\n",
    "    sampledInd = np.array(sampledIndices)\n",
    "    pool.indicesKnown = np.concatenate(([pool.indicesKnown, np.array(sampledInd)]))\n",
    "\n",
    "    pool.indicesUnknown = np.array([])\n",
    "    for i in poolDataIndices:\n",
    "        if i not in pool.indicesKnown:\n",
    "            pool.indicesUnknown = np.concatenate(([pool.indicesUnknown, np.array([i])])); \n",
    "\n",
    "    training_data = poolData.loc[poolData[iID].isin(pool.indicesKnown)].reset_index(drop=True)\n",
    "    train_data_idx = poolData.index[poolData[iID].isin(pool.indicesKnown)].tolist()\n",
    "    train_data = poolDataEmb_train[train_data_idx]\n",
    "    train_labels = np.array(training_data[goldLabel].tolist())\n",
    "    #print(\"Start training.\")\n",
    "    model.fit(train_data, train_labels) \n",
    "\n",
    "    y_pred_train = model.predict(train_data)\n",
    "    logits_train = model.predict_proba(train_data)\n",
    "    probs_train = np.array(logits_train)\n",
    "\n",
    "\n",
    "    y_pred_val = model.predict(poolDataEmb_val)\n",
    "    logits_val = model.predict_proba(poolDataEmb_val)\n",
    "    probs_val = np.array(logits_val)\n",
    "    val_labels = np.array(validation_data[goldLabel].tolist())\n",
    "\n",
    "    y_pred_test = model.predict(poolDataEmb_test)\n",
    "    logits_test = model.predict_proba(poolDataEmb_test)\n",
    "    probs_test = np.array(logits_test)\n",
    "    test_labels = np.array(test_data[testGoldLabel].tolist())\n",
    "\n",
    "    # check if binary or multi class classification\n",
    "    num_classes = len(set(val_labels))\n",
    "    if num_classes == 2:\n",
    "        average = 'binary'\n",
    "    else:\n",
    "        average = 'macro'\n",
    "\n",
    "    sampledItems = ''.join(str(e)+' ' for e in sampledIndices)\n",
    "\n",
    "    pre_train, rec_train, f1_train, _ = precision_recall_fscore_support(train_labels, y_pred_train, average=average, beta=1)\n",
    "    ece_train = ece_score(train_labels, probs_train)\n",
    "    _, _, f01_train, _ = precision_recall_fscore_support(train_labels, y_pred_train, average=average, beta=0.1)\n",
    "    _, _, f10_train, _ = precision_recall_fscore_support(train_labels, y_pred_train, average=average, beta=10)\n",
    "\n",
    "\n",
    "    pre_val, rec_val, f1_val, _ = precision_recall_fscore_support(val_labels, y_pred_val, average=average, beta=1)\n",
    "    ece_val = ece_score(val_labels, probs_val)\n",
    "    _, _, f01_val, _ = precision_recall_fscore_support(val_labels, y_pred_val, average=average, beta=0.1)\n",
    "    _, _, f10_val, _ = precision_recall_fscore_support(val_labels, y_pred_val, average=average, beta=10)\n",
    "\n",
    "    pre_test, rec_test, f1_test, _ = precision_recall_fscore_support(test_labels, y_pred_test, average=average, beta=1)\n",
    "    ece_test = ece_score(test_labels, probs_test)\n",
    "    _, _, f01_test, _ = precision_recall_fscore_support(test_labels, y_pred_test, average=average, beta=0.1)\n",
    "    _, _, f10_test, _ = precision_recall_fscore_support(test_labels, y_pred_test, average=average, beta=10)\n",
    "\n",
    "    if average == 'binary':\n",
    "        brier_train = brier_score_loss(train_labels, probs_train[:,1])\n",
    "        brier_val = brier_score_loss(val_labels, probs_val[:,1])\n",
    "        brier_test = brier_score_loss(test_labels, probs_test[:,1])\n",
    "\n",
    "        print(\n",
    "            'Iteration: {}. F1: {:1.3f}, Precision: {:1.3f}, Recall: {:1.3f}'.\n",
    "             format(alBatch, f1_val, pre_val, rec_val))\n",
    "        # print to result file\n",
    "        with open(res_path, 'a') as f:\n",
    "            res_i = '{}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}\\n'.format(alBatch, sampledItems, pre_train, rec_train, f01_train, f1_train, f10_train, ece_train, brier_train, pre_val, rec_val, f01_val, f1_val, f10_val, ece_val, brier_val, pre_test, rec_test, f01_test, f1_test, f10_test, ece_test, brier_test)\n",
    "            f.write(res_i)\n",
    "    else:\n",
    "    \n",
    "        print(\n",
    "            'Iteration: {}. F1: {:1.3f}, Precision: {:1.3f}, Recall: {:1.3f}'.\n",
    "             format(alBatch, f1_val, pre_val, rec_val))\n",
    "        # print to result file\n",
    "        with open(res_path, 'a') as f:\n",
    "            res_i = '{}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}\\n'.format(alBatch, sampledItems, pre_train, rec_train, f01_train, f1_train, f10_train, ece_train, pre_val, rec_val, f01_val, f1_val, f10_val, ece_val, pre_test, rec_test, f01_test, f1_test, f10_test, ece_test)\n",
    "            f.write(res_i)\n",
    "\n",
    "if al_strategy == 'diversity':\n",
    "    divRanking = pd.DataFrame(\n",
    "       {'sampleID': sampleIds,\n",
    "         'diversityRank': samplingRanks\n",
    "       })\n",
    "\n",
    "    divRanking.to_csv(resDiversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ALforCalibration_MLP_mclass.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
