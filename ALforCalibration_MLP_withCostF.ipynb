{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "en5IRda4XIVp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/fabio.casati/.venvs/389/lib/python3.8/site-packages (4.6.1)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/fabio.casati/.venvs/389/lib/python3.8/site-packages (from transformers) (4.60.0)\r\n",
      "Requirement already satisfied: filelock in /Users/fabio.casati/.venvs/389/lib/python3.8/site-packages (from transformers) (3.0.12)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/fabio.casati/.venvs/389/lib/python3.8/site-packages (from transformers) (1.20.3)\r\n",
      "Requirement already satisfied: huggingface-hub==0.0.8 in /Users/fabio.casati/.venvs/389/lib/python3.8/site-packages (from transformers) (0.0.8)\r\n",
      "Requirement already satisfied: sacremoses in /Users/fabio.casati/.venvs/389/lib/python3.8/site-packages (from transformers) (0.0.45)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/fabio.casati/.venvs/389/lib/python3.8/site-packages (from transformers) (2021.4.4)\r\n",
      "Requirement already satisfied: packaging in /Users/fabio.casati/.venvs/389/lib/python3.8/site-packages (from transformers) (20.9)\r\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /Users/fabio.casati/.venvs/389/lib/python3.8/site-packages (from transformers) (0.10.3)\r\n",
      "Requirement already satisfied: requests in /Users/fabio.casati/.venvs/389/lib/python3.8/site-packages (from transformers) (2.25.1)\r\n",
      "Requirement already satisfied: six in /Users/fabio.casati/.venvs/389/lib/python3.8/site-packages (from sacremoses->transformers) (1.16.0)\r\n",
      "Requirement already satisfied: joblib in /Users/fabio.casati/.venvs/389/lib/python3.8/site-packages (from sacremoses->transformers) (1.0.1)\r\n",
      "Requirement already satisfied: click in /Users/fabio.casati/.venvs/389/lib/python3.8/site-packages (from sacremoses->transformers) (8.0.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/fabio.casati/.venvs/389/lib/python3.8/site-packages (from packaging->transformers) (2.4.7)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/fabio.casati/.venvs/389/lib/python3.8/site-packages (from requests->transformers) (2.10)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/fabio.casati/.venvs/389/lib/python3.8/site-packages (from requests->transformers) (4.0.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/fabio.casati/.venvs/389/lib/python3.8/site-packages (from requests->transformers) (2020.12.5)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/fabio.casati/.venvs/389/lib/python3.8/site-packages (from requests->transformers) (1.26.4)\r\n",
      "\u001B[33mWARNING: You are using pip version 20.2.3; however, version 21.1.2 is available.\r\n",
      "You should consider upgrading via the '/Users/fabio.casati/.venvs/389/bin/python3 -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Requirement already satisfied: sklearn in /Users/fabio.casati/.venvs/389/lib/python3.8/site-packages (0.0)\r\n",
      "Requirement already satisfied: scikit-learn in /Users/fabio.casati/.venvs/389/lib/python3.8/site-packages (from sklearn) (0.24.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/fabio.casati/.venvs/389/lib/python3.8/site-packages (from scikit-learn->sklearn) (2.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/fabio.casati/.venvs/389/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.20.3)\r\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/fabio.casati/.venvs/389/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.6.3)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/fabio.casati/.venvs/389/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.0.1)\r\n",
      "\u001B[33mWARNING: You are using pip version 20.2.3; however, version 21.1.2 is available.\r\n",
      "You should consider upgrading via the '/Users/fabio.casati/.venvs/389/bin/python3 -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Requirement already satisfied: netcal in /Users/fabio.casati/.venvs/389/lib/python3.8/site-packages (1.1.3)\r\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /Users/fabio.casati/.venvs/389/lib/python3.8/site-packages (from netcal) (0.24.2)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/fabio.casati/.venvs/389/lib/python3.8/site-packages (from netcal) (1.20.3)\r\n",
      "Requirement already satisfied: torch>=1.1 in /Users/fabio.casati/.venvs/389/lib/python3.8/site-packages (from netcal) (1.8.1)\r\n",
      "Requirement already satisfied: scipy>=1.3 in /Users/fabio.casati/.venvs/389/lib/python3.8/site-packages (from netcal) (1.6.3)\r\n",
      "Requirement already satisfied: tqdm in /Users/fabio.casati/.venvs/389/lib/python3.8/site-packages (from netcal) (4.60.0)\r\n",
      "Requirement already satisfied: matplotlib>=3.1 in /Users/fabio.casati/.venvs/389/lib/python3.8/site-packages (from netcal) (3.4.2)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/fabio.casati/.venvs/389/lib/python3.8/site-packages (from scikit-learn>=0.20.0->netcal) (1.0.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/fabio.casati/.venvs/389/lib/python3.8/site-packages (from scikit-learn>=0.20.0->netcal) (2.1.0)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/fabio.casati/.venvs/389/lib/python3.8/site-packages (from torch>=1.1->netcal) (3.10.0.0)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/fabio.casati/.venvs/389/lib/python3.8/site-packages (from matplotlib>=3.1->netcal) (8.2.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/fabio.casati/.venvs/389/lib/python3.8/site-packages (from matplotlib>=3.1->netcal) (1.3.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/fabio.casati/.venvs/389/lib/python3.8/site-packages (from matplotlib>=3.1->netcal) (2.8.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/fabio.casati/.venvs/389/lib/python3.8/site-packages (from matplotlib>=3.1->netcal) (0.10.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/fabio.casati/.venvs/389/lib/python3.8/site-packages (from matplotlib>=3.1->netcal) (2.4.7)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/fabio.casati/.venvs/389/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib>=3.1->netcal) (1.16.0)\r\n",
      "\u001B[33mWARNING: You are using pip version 20.2.3; however, version 21.1.2 is available.\r\n",
      "You should consider upgrading via the '/Users/fabio.casati/.venvs/389/bin/python3 -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install sklearn\n",
    "!pip install netcal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hwU1cqCKXmSi"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from random import shuffle\n",
    "\n",
    "from netcal.metrics import ECE\n",
    "from sklearn.metrics import precision_recall_fscore_support, brier_score_loss, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RPfLOR2CKB6Z"
   },
   "outputs": [],
   "source": [
    "# please delete this cell if you don't run on Colab\n",
    "## Mount Drive into Colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/Users/fabio.casati/Documents/dev/data/'\n",
    "records_file_name = 'incident_hi8_aug2019.csv'\n",
    "text_feature = 'short_description'                                            # define the name of column where you keep the items \n",
    "label = 'assignment_group'                                      # define the name of column where you keep the gold labels of your test data\n",
    "categorical_features = []\n",
    "\n",
    "features = [text_feature] + categorical_features \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_filename = folder + records_file_name\n",
    "log_filename = folder + 'logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabio.casati/.venvs/389/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3169: DtypeWarning: Columns (10,20,67,116,117,136,141,160,182) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "data": {
      "text/plain": "(21515, 2)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(records_filename)\n",
    "df = df[features + [label]]\n",
    "df = df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "edf = df.sample(20)\n",
    "esdf = edf[['short_description', 'assignment_group']]\n",
    "\n",
    "def hash_string_tokens(s):\n",
    "    tokens = s.split()[:5]\n",
    "    htokens =[str(hash(t)) for t in tokens]\n",
    "    return ' '.join(htokens)\n",
    "\n",
    "for c in esdf.columns:\n",
    "    esdf[c] = esdf[c].str[:20]\n",
    "esdf['assignment_group'] = esdf['assignment_group'].str[:10]\n",
    "esdf['label'] = esdf['assignment_group'].apply(lambda x: str(hash(x)))\n",
    "esdf['text_feature'] = esdf['short_description'].apply(lambda x: hash_string_tokens(x))\n",
    "\n",
    "esdf[['text_feature','label']].to_csv('burcu_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[label] = df[label].astype('str')\n",
    "s = df[label].value_counts()\n",
    "s = s[s>1]\n",
    "df = df[df[label].isin(s.index)]\n",
    "df[label].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['binary_label']= False\n",
    "df.loc[df[label]=='Maintenance Windows', 'binary_label'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'binary_label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest  = train_test_split(df[features], df[label], random_state = 1, stratify = df[label])\n",
    "xtrain_noval, xval, ytrain_noval, yval  = train_test_split(xtrain, ytrain, random_state = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "iQmf-qpKpICj"
   },
   "outputs": [],
   "source": [
    "#please define all the configurations for your dataset in this cell, and then run the code\n",
    "#configurations \n",
    "# res_path                = 'drive/My Drive/Colab Notebooks/deepALForCalibration/res/withCost/'                            # specify the path to keep results\n",
    "# data_folder             = 'drive/My Drive/Colab Notebooks/deepALForCalibration/datasets/binary/disaster_relevance/'  #specify the path to the folder where you keep your datasets\n",
    "# dataToTrain             = '2_train_indexed_disaster_relevance_binary.csv'    # file name for your training data\n",
    "# dataToVal               = '2_val_indexed_disaster_relevance_binary.csv'      # file name for your validation data\n",
    "# dataToTest              = '2_test_indexed_disaster_relevance_binary.csv'     # file name for your test data\n",
    "# logfile_name            = \"2-disasterRelevance-MLP3\"                         # specify the name of the result file\n",
    "#al_strategy             = 'random'                                          # specify the active learning strategy you want to use; 'random', 'diversity', or 'uncertainty'\n",
    "al_strategies           = ['random', 'uncertainty', 'diversity']\n",
    "minimum_training_items  = 157                                                # minimum number of training items before we first train a model\n",
    "alBatchNum              = 10                                                 # define the total number of batches in active learning pipeline\n",
    "alBatchSize             = 740                                                # define the size of one batch in active learning pipeline\n",
    "cfpList                     = [1]                                            # define the cost of obtaining a false positive\n",
    "cfnList                     = [1, 10, 100, 1000]                             # define the cost of obtaining a false negative\n",
    "chList                      = [1, 10, 100, 1000]                             # cost of asking humans\n",
    "\n",
    "# columns of the csv file used in the experiments: text/content for each item, gold labels for each item, confidence scores for each class, ID of each item \n",
    "# specify the column names of your data\n",
    "# iID                     = 'itemID'                                          # give each item an ID, it will be used during active learning\n",
    "# goldLabel               = 'crowd_label'                                     # define the name of column where you keep the gold labels of your data\n",
    "\n",
    "\n",
    "#Ml Pipeline\n",
    "maxTfIdfFeat            = 1024                                               # define the maximum number of features for tfidf \n",
    "\n",
    "hidden_layer_sizes = (200, 150, 100 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df= 2, max_features = 1000, ngram_range = (1, 2))\n",
    "model = MLPClassifier(hidden_layer_sizes = hidden_layer_sizes, max_iter=100)  # define the model parameters that best fit to your data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation: train vectorizer on the entire training data (except validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=1000, min_df=2, ngram_range=(1, 2))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(xtrain_noval[text_feature])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get initial batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xtrain_noval_start = xtrain_noval[:100]\n",
    "ytrain_noval_start = ytrain_noval[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ytrain_noval_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.transform(Xdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_threshold = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(200, 150, 100), max_iter=100)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xdf = xtrain_noval_start\n",
    "y = ytrain_noval_start\n",
    "\n",
    "X = vectorizer.transform(Xdf)\n",
    "model.fit(X, y) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess model quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfp=1\n",
    "cfn=1\n",
    "ch=1\n",
    "\n",
    "# on train\n",
    "y_hat_w_humans_options, y_hat = classify(model, vectorizer, Xdf, y, cfp, cfn, ch)\n",
    "\n",
    "\n",
    "# on val\n",
    "classify(model, vectorizer, xval, yval, cfp, cfn, ch)\n",
    "\n",
    "\n",
    "\n",
    "# on test\n",
    "\n",
    "y_hat_w_humans_options[:10], y_hat[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_hat_proba[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_aware_prediction(model, vectorizer, Xtestdf):\n",
    "    Xtestdf = xval[text_feature]\n",
    "    y_hat_proba = model.predict_proba(vectorizer.transform(Xtestdf))\n",
    "    y_hat = y_hat_proba[:,1] > clf_threshold\n",
    "    return y_hat_proba, y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(model, vectorizer, Xtestdf, goldLabels, cfp, cfn, ch):\n",
    "    Xtestdf = xval[text_feature]\n",
    "    y_hat_proba = model.predict_proba(vectorizer.transform(Xtestdf))\n",
    "#     y_hat_proba[:,1]\n",
    "    y_clf = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for i in range(len(y_hat_proba)):\n",
    "        p = y_hat_proba[i][1]  #probability of being positive\n",
    "        if ((cfp*(1-p)) < (cfn*p)) and ((cfp*(1-p)) < ch):\n",
    "            y_pred.append(1)\n",
    "            y_clf.append(1)\n",
    "        elif ((cfn*p) < (cfp*(1-p))) and ((cfn*p) < ch):\n",
    "            y_pred.append(0)\n",
    "            y_clf.append(0)\n",
    "        else:\n",
    "            y_clf.append(-1)\n",
    "            y_pred.append(goldLabels[i])\n",
    "    return y_clf, np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_w_humans_options, y_hat= classify(model, vectorizer, xval[text_feature], yval, 1,1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4032,)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = cost_aware_prediction(model, vectorizer,xval[text_feature], clf_threshold)\n",
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4032,)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yval.to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1980,    4,    2, 2046])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(yval.to_numpy(), y_hat).ravel()\n",
    "cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tn, fp, fn, tp = cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateCost(cm):\n",
    "    tn, fp, fn, tp = cm\n",
    "    uc = y_clf.count(-1)\n",
    "    cost = (cfp*fp + cfn*fn + ch*uc) / len(y_clf) sum(tn, fp, fn, tp)\n",
    "    return cost, uc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the total cost of predictions\n",
    "def calculateCost(fp, fn, y_clf):\n",
    "    uc = y_clf.count(-1)\n",
    "    cost = (cfp*fp + cfn*fn + ch*uc) / len(y_clf)\n",
    "    return cost, uc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, vectorizer, train_data, train_labels, poolDataEmb_val, validation_data, poolDataEmb_test, test_data, mClass, sampledIndices, res_path, alBatch):\n",
    "\n",
    "    logits_train = model.predict_proba(train_data)\n",
    "    probs_train = np.array(logits_train)\n",
    "    y_clf_train, y_pred_train = classify(probs_train, train_labels)\n",
    "\n",
    "    logits_val = model.predict_proba(poolDataEmb_val)\n",
    "    probs_val = np.array(logits_val)\n",
    "    val_labels = np.array(validation_data[goldLabel].tolist())\n",
    "    y_clf_val, y_pred_val = classify(probs_val, val_labels)\n",
    "\n",
    "    logits_test = model.predict_proba(poolDataEmb_test)\n",
    "    probs_test = np.array(logits_test)\n",
    "    test_labels = np.array(test_data[testGoldLabel].tolist())\n",
    "    y_clf_test, y_pred_test = classify(probs_test, test_labels)\n",
    "\n",
    "    # check if binary or multi class classification\n",
    "    if len(mClass) == 2:\n",
    "        average = 'binary'\n",
    "    else:\n",
    "        average = 'macro'\n",
    "\n",
    "    sampledItems = ''.join(str(e)+' ' for e in sampledIndices)\n",
    "\n",
    "    pre_train, rec_train, f1_train, _ = precision_recall_fscore_support(train_labels, y_pred_train, average=average, beta=1)\n",
    "    \n",
    "    ece_train = ece_score(train_labels, probs_train)\n",
    "    _, _, f01_train, _ = precision_recall_fscore_support(train_labels, y_pred_train, average=average, beta=0.1)\n",
    "    _, _, f10_train, _ = precision_recall_fscore_support(train_labels, y_pred_train, average=average, beta=10)\n",
    "\n",
    "    pre_val, rec_val, f1_val, _ = precision_recall_fscore_support(val_labels, y_pred_val, average=average, beta=1)\n",
    "    ece_val = ece_score(val_labels, probs_val)\n",
    "    _, _, f01_val, _ = precision_recall_fscore_support(val_labels, y_pred_val, average=average, beta=0.1)\n",
    "    _, _, f10_val, _ = precision_recall_fscore_support(val_labels, y_pred_val, average=average, beta=10)\n",
    "\n",
    "    pre_test, rec_test, f1_test, _ = precision_recall_fscore_support(test_labels, y_pred_test, average=average, beta=1)\n",
    "    ece_test = ece_score(test_labels, probs_test)\n",
    "    _, _, f01_test, _ = precision_recall_fscore_support(test_labels, y_pred_test, average=average, beta=0.1)\n",
    "    _, _, f10_test, _ = precision_recall_fscore_support(test_labels, y_pred_test, average=average, beta=10)\n",
    "\n",
    "    if average == 'binary':\n",
    "        brier_train = brier_score_loss(train_labels, probs_train[:,1])\n",
    "        brier_val = brier_score_loss(val_labels, probs_val[:,1])\n",
    "        brier_test = brier_score_loss(test_labels, probs_test[:,1])\n",
    "\n",
    "        tn_train, fp_train, fn_train, tp_train = confusion_matrix(train_labels, y_pred_train).ravel()\n",
    "        tn_val, fp_val, fn_val, tp_val = confusion_matrix(val_labels, y_pred_val).ravel()\n",
    "        tn_test, fp_test, fn_test, tp_test = confusion_matrix(test_labels, y_pred_test).ravel()\n",
    "\n",
    "        cost_train, uc_train = calculateCost(fp_train, fn_train, y_clf_train)\n",
    "        cost_val, uc_val = calculateCost(fp_val, fn_val, y_clf_val)\n",
    "        cost_test, uc_test = calculateCost(fp_test, fn_test, y_clf_test)\n",
    "        \n",
    "        print(\n",
    "            'Iteration: {}. F1: {:1.3f}, Precision: {:1.3f}, Recall: {:1.3f}'.\n",
    "            format(alBatch, f1_val, pre_val, rec_val))\n",
    "        # print to result file\n",
    "        with open(res_path, 'a') as f:\n",
    "            res_i = '{}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}\\n'.format(alBatch, sampledItems, pre_train, rec_train, f01_train, f1_train, f10_train, ece_train, brier_train, cost_train, uc_train, pre_val, rec_val, f01_val, f1_val, f10_val, ece_val, brier_val, cost_val, uc_val, pre_test, rec_test, f01_test, f1_test, f10_test, ece_test, brier_test, cost_test, uc_test)\n",
    "            f.write(res_i)\n",
    "    else:\n",
    "\n",
    "        print(\n",
    "            'Iteration: {}. F1: {:1.3f}, Precision: {:1.3f}, Recall: {:1.3f}'.\n",
    "            format(alBatch, f1_val, pre_val, rec_val))\n",
    "        # print to result file\n",
    "        with open(res_path, 'a') as f:\n",
    "            res_i = '{}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}\\n'.format(alBatch, sampledItems, pre_train, rec_train, f01_train, f1_train, f10_train, ece_train, pre_val, rec_val, f01_val, f1_val, f10_val, ece_val, pre_test, rec_test, f01_test, f1_test, f10_test, ece_test)\n",
    "            f.write(res_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "initial_batch\n",
    "\n",
    "Xtest = \n",
    "\n",
    "\n",
    "# X = prepare_features(dt[txt].tolist(), min_df= 0, max_features = maxTfIdfFeat, ngram_range = (1, 3))\n",
    "# self.data = dt\n",
    "# self.poolDataEmb = X\n",
    "# self.poolGoldLabels = y\n",
    "# self.mClass = list(set(self.poolGoldLabels.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def setStartState(minimum_training_items):\n",
    "#     ''' This functions creates the initial training set which contains the equal number of samples per class\n",
    "#     Input:\n",
    "#     nStart -- number of labelled datapoints (size of training set)\n",
    "#     '''\n",
    "# #     self.nStart = nStart\n",
    "# #     data = self.data.copy()\n",
    "#     # get predefined points so that all classes are represented and initial classifier could be trained.\n",
    "#     sampledIndices = []\n",
    "#     for cls in self.mClass:\n",
    "#         indices = np.array(np.where(self.poolGoldLabels == cls)).tolist()[0]\n",
    "#         idx = random.sample(indices, nStart // len(mClass))\n",
    "#         sampledIndices = sampledIndices + idx\n",
    "\n",
    "#     sData = data.iloc[sampledIndices]\n",
    "#     self.labeledSet = sData.reset_index(drop=True)\n",
    "#     droppedData = data.drop(sampledIndices)\n",
    "#     self.unlabeledSet = droppedData.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KH4SinJeXRWo"
   },
   "outputs": [],
   "source": [
    "poolDataEmb_train = np.array([])\n",
    "poolDataEmb_val = np.array([])\n",
    "poolDataEmb_test = np.array([])\n",
    "\n",
    "# data directories\n",
    "unlabeled_data_dir = data_folder + dataToTrain\n",
    "validation_data_dir = data_folder + dataToVal\n",
    "test_data_dir = data_folder + dataToTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pVYyaN0_Z5Fd"
   },
   "outputs": [],
   "source": [
    "class DiversitySampling():\n",
    "\n",
    "    def __init__(self, verbose):\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def get_validation_rankings(self, model, validation_data, val_emb):\n",
    "        \"\"\"Get model outliers from unlabeled data \n",
    "    \n",
    "        Keyword arguments:\n",
    "            model -- current Machine Learning model for this task\n",
    "            unlabeled_data -- data that does not yet have a label\n",
    "            validation_data -- held out data drawn from the same distribution as the training data\n",
    "            number -- number of items to sample\n",
    "            limit -- sample from only this many items for faster sampling (-1 = no limit)\n",
    "    \n",
    "        An outlier is defined as \n",
    "        unlabeled_data with the lowest average from rank order of logits\n",
    "        where rank order is defined by validation data inference \n",
    "    \n",
    "        \"\"\"\n",
    "                \n",
    "        validation_rankings = [] # 2D array, every neuron by ordered list of output on validation data per neuron    \n",
    "    \n",
    "        # Get per-neuron scores from validation data\n",
    "        if self.verbose:\n",
    "            print(\"Getting neuron activation scores from validation data\")\n",
    "\n",
    "        pred = model.predict_proba(val_emb) \n",
    "\n",
    "        v = 0\n",
    "        for neuron_outputs in pred:\n",
    "            # initialize array if we haven't yet\n",
    "            if len(validation_rankings) == 0:\n",
    "                for output in list(neuron_outputs):\n",
    "                    validation_rankings.append([0.0] * len(validation_data))\n",
    "\n",
    "            n=0\n",
    "            for output in list(neuron_outputs):\n",
    "                validation_rankings[n][v] = output\n",
    "                n += 1\n",
    "            v +=1\n",
    "            \n",
    "        \n",
    "        # Rank-order the validation scores \n",
    "        v=0\n",
    "        for validation in validation_rankings:\n",
    "            validation.sort() \n",
    "            validation_rankings[v] = validation\n",
    "            v += 1\n",
    "          \n",
    "        return validation_rankings \n",
    "    \n",
    "    def get_rank(self, value, rankings):\n",
    "        \"\"\" get the rank of the value in an ordered array as a percentage \n",
    "    \n",
    "        Keyword arguments:\n",
    "            value -- the value for which we want to return the ranked value\n",
    "            rankings -- the ordered array in which to determine the value's ranking\n",
    "        \n",
    "        returns linear distance between the indexes where value occurs, in the\n",
    "        case that there is not an exact match with the ranked values    \n",
    "        \"\"\"\n",
    "        \n",
    "        index = 0 # default: ranking = 0\n",
    "        \n",
    "        for ranked_number in rankings:\n",
    "            if value < ranked_number:\n",
    "                break #NB: this O(N) loop could be optimized to O(log(N))\n",
    "            index += 1        \n",
    "        \n",
    "        if(index >= len(rankings)):\n",
    "            index = len(rankings) # maximum: ranking = 1\n",
    "            \n",
    "        elif(index > 0):\n",
    "            # get linear interpolation between the two closest indexes \n",
    "            \n",
    "            diff = rankings[index] - rankings[index - 1]\n",
    "            perc = value - rankings[index - 1]\n",
    "            linear = perc / diff\n",
    "            index = float(index - 1) + linear\n",
    "        \n",
    "        absolute_ranking = index / len(rankings)\n",
    "    \n",
    "        return(absolute_ranking)\n",
    "    \n",
    "    def get_model_outliers(self, dataPool, model, unlabeled_data, unl_emb, validation_data, val_emb, number):\n",
    "        \"\"\"Get model outliers from unlabeled data \n",
    "    \n",
    "        Keyword arguments:\n",
    "            model -- current Machine Learning model for this task\n",
    "            unlabeled_data -- data that does not yet have a label\n",
    "            validation_data -- held out data drawn from the same distribution as the training data\n",
    "            number -- number of items to sample\n",
    "            limit -- sample from only this many items for faster sampling (-1 = no limit)\n",
    "    \n",
    "        An outlier is defined as \n",
    "        unlabeled_data with the lowest average from rank order of logits\n",
    "        where rank order is defined by validation data inference \n",
    "    \n",
    "        \"\"\"\n",
    "    \n",
    "        # Get per-neuron scores from validation data\n",
    "        validation_rankings = self.get_validation_rankings(model, validation_data, val_emb)\n",
    "\n",
    "        # Iterate over unlabeled items\n",
    "        if self.verbose:\n",
    "            print(\"Getting rankings for unlabeled data\")\n",
    "    \n",
    "        outliers = []\n",
    "        pred = model.predict_proba(unl_emb) \n",
    "\n",
    "        itID = 0\n",
    "        for neuron_outputs in pred:\n",
    "            n=0\n",
    "            ranks = []\n",
    "            for output in neuron_outputs:\n",
    "                rank = self.get_rank(output, validation_rankings[n])\n",
    "                ranks.append(rank)\n",
    "                n += 1 \n",
    "            avgRank = 1 - (sum(ranks) / len(neuron_outputs)) # average rank\n",
    "            currentRow = unlabeled_data.iloc[[itID]].reset_index(drop=True)\n",
    "            rowIndex = currentRow.itemID.item()\n",
    "            row = dataPool.loc[dataPool[iID] == rowIndex]\n",
    "            row['avgRank'] = avgRank\n",
    "            outliers.append(row.values.flatten().tolist()) \n",
    "            itID += 1\n",
    "        outliers.sort(reverse=True, key=lambda x: x[-1])       \n",
    "        return outliers[:number:]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g-pASfloTKzi"
   },
   "outputs": [],
   "source": [
    "def random_sampling(dataIds, nQuery):\n",
    "    '''Randomly samples the points'''\n",
    "    query_idx = random.sample(range(len(dataIds)), nQuery)\n",
    "    selectedIndex = dataIds[query_idx]\n",
    "    return selectedIndex\n",
    "        \n",
    "def uncertainty_sampling(model, unl_emb, number):\n",
    "    '''Points are sampled according to uncertainty sampling criterion'''\n",
    "\n",
    "    pred = model.predict_proba(unl_emb)\n",
    "    uncertainty_scores = 1 - pred.max(axis=1)\n",
    "    score_indices = np.argsort(uncertainty_scores)\n",
    "    return score_indices[-number:]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_4EwXlvgazxW"
   },
   "outputs": [],
   "source": [
    "# ## Feature Preparation\n",
    "# def prepare_features(X_train, min_df=2, max_features=None, ngram_range=(1, 3)):\n",
    "#     # compute tfidf features\n",
    "#     tfidf = TfidfVectorizer(min_df=min_df, max_features=max_features,\n",
    "#                 strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n",
    "#                 ngram_range=ngram_range, use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "#                 stop_words=None, lowercase=False)\n",
    "\n",
    "#     X_train_tfidf = tfidf.fit_transform(X_train).toarray()\n",
    "#     return X_train_tfidf\n",
    "\n",
    "class Data():\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        \n",
    "        # each dataset will have a pool of data, together with their IDs and gold labels \n",
    "#         self.poolData = np.array([])\n",
    "#         self.poolGoldLabels = np.array([])\n",
    "        \n",
    "#         dt = pd.read_csv(filename)\n",
    "#         dt = dt.dropna()\n",
    "#         dt = dt.drop_duplicates()\n",
    "# #         dt = dt.reset_index(drop=True)\n",
    "#         y = dt[goldLabel].values\n",
    "#         X = prepare_features(dt[txt].tolist(), min_df= 0, max_features = maxTfIdfFeat, ngram_range = (1, 3))\n",
    "#         self.data = dt\n",
    "#         self.poolDataEmb = X\n",
    "#         self.poolGoldLabels = y\n",
    "#         self.mClass = list(set(self.poolGoldLabels.tolist()))\n",
    "        \n",
    "    def setStartState(self, nStart):\n",
    "        ''' This functions creates the initial training set which contains the equal number of samples per class\n",
    "        Input:\n",
    "        nStart -- number of labelled datapoints (size of training set)\n",
    "        '''\n",
    "        self.nStart = nStart\n",
    "        data = self.data.copy()\n",
    "        # get predefined points so that all classes are represented and initial classifier could be trained.\n",
    "        sampledIndices = []\n",
    "        for cls in self.mClass:\n",
    "            indices = np.array(np.where(self.poolGoldLabels == cls)).tolist()[0]\n",
    "            idx = random.sample(indices, nStart // len(mClass))\n",
    "            sampledIndices = sampledIndices + idx\n",
    "\n",
    "        sData = data.iloc[sampledIndices]\n",
    "        self.labeledSet = sData.reset_index(drop=True)\n",
    "        droppedData = data.drop(sampledIndices)\n",
    "        self.unlabeledSet = droppedData.reset_index(drop=True)\n",
    "\n",
    "# function to calculate the ECE score\n",
    "def ece_score(y_true, y_prob, n_bins=10):\n",
    "    ece = ECE(n_bins)\n",
    "    ece_val = ece.measure(y_prob, y_true)\n",
    "\n",
    "    return ece_val\n",
    "\n",
    "# classify by the threshold with respect to cost of different errors\n",
    "\n",
    "\n",
    "#calculate the total cost of predictions\n",
    "def calculateCost(fp, fn, y_clf):\n",
    "    uc = y_clf.count(-1)\n",
    "    cost = (cfp*fp + cfn*fn + ch*uc) / len(y_clf)\n",
    "    return cost, uc\n",
    "\n",
    "#evaluate the trained model on test/validation/training sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ieL71IeGa1VT"
   },
   "outputs": [],
   "source": [
    "#load datasets\n",
    "pool = Data(unlabeled_data_dir)\n",
    "mClass =  pool.mClass\n",
    "pool.setStartState(minimum_training_items)\n",
    "\n",
    "validation = Data(validation_data_dir)\n",
    "validation_data = validation.data\n",
    "test = Data(test_data_dir)\n",
    "test_data = test.data\n",
    "\n",
    "for al_strategy in al_strategies:\n",
    "    for cfp in cfpList:\n",
    "        for cfn in cfnList:\n",
    "            for ch in chList:\n",
    "                poolData = pool.data\n",
    "                training_data = pool.labeledSet\n",
    "                unlabeled_data = pool.unlabeledSet\n",
    "\n",
    "                poolDataEmb_val = validation.poolDataEmb\n",
    "                poolDataEmb_test = test.poolDataEmb\n",
    "\n",
    "                train_data = pool.poolDataEmb[poolData.index[poolData[iID].isin(training_data[iID].values)].tolist()]\n",
    "                train_labels = np.array(training_data[goldLabel].tolist())\n",
    "\n",
    "                #Start active learning\n",
    "                sampleIds = []\n",
    "                samplingRanks = []\n",
    "\n",
    "                log_name = al_strategy + '_' + logfile_name + \"_cfp_{}_cfn_{}_ch_{}.csv\".format(cfp, cfn, ch)\n",
    "\n",
    "                # create log file\n",
    "                log_path = res_path + log_name\n",
    "                if len(mClass) == 2:\n",
    "                    with open(log_path, 'w') as f:\n",
    "                        c = 'alBatch, sampledIndices, pre_train, rec_train, f01_train, f1_train, f10_train, ece_train, brier_train, cost_train, uc_train, pre_val, rec_val, f01_val, f1_val, f10_val, ece_val, brier_val, cost_val, uc_val, pre_test, rec_test, f01_test, f1_test, f10_test, ece_test, brier_test, cost_test, uc_test'\n",
    "                        f.write(c + '\\n')\n",
    "                else:\n",
    "                    with open(log_path, 'w') as f:\n",
    "                        c = 'alBatch, sampledIndices, pre_train, rec_train, f01_train, f1_train, f10_train, ece_train, pre_val, rec_val, f01_val, f1_val, f10_val, ece_val, pre_test, rec_test, f01_test, f1_test, f10_test, ece_test'\n",
    "                        f.write(c + '\\n')\n",
    "\n",
    "                model.fit(train_data, train_labels) \n",
    "                evaluate(train_data, train_labels, poolDataEmb_val, validation_data, poolDataEmb_test, test_data, mClass, [], log_path, 0)\n",
    "\n",
    "                for alBatch in range(alBatchNum):\n",
    "                    sampledIndices = []\n",
    "\n",
    "                    unl_dataEmb = pool.poolDataEmb[poolData.index[poolData[iID].isin(unlabeled_data[iID].values)].tolist()]\n",
    "\n",
    "                    if al_strategy == 'diversity':\n",
    "                        strategy = DiversitySampling(True)\n",
    "                        sampledItems = strategy.get_model_outliers(poolData, model, unlabeled_data, unl_dataEmb, validation_data, poolDataEmb_val, number=alBatchSize) \n",
    "        \n",
    "                        for outlier in sampledItems:\n",
    "                            samplingRanks.append(outlier[-1])\n",
    "                            sampleIds.append(outlier[-2])\n",
    "                            sampledIndices.append(outlier[-2])\n",
    "   \n",
    "                    elif al_strategy == 'random':\n",
    "                        sampledIndices = random_sampling(unlabeled_data[iID].values, alBatchSize)\n",
    "                        for i in sampledIndices: sampleIds.append(i)\n",
    "                    elif al_strategy == 'uncertainty':\n",
    "                        idx = uncertainty_sampling(model, unl_dataEmb, alBatchSize)\n",
    "                        sampledIndices = unlabeled_data.loc[idx][iID].tolist()\n",
    "                        for i in sampledIndices: sampleIds.append(i)\n",
    "                    else:\n",
    "                        # random sampling by default\n",
    "                        sampledIndices = random_sampling(unlabeled_data[iID].values, alBatchSize)\n",
    "                        for i in sampledIndices: sampleIds.append(i)\n",
    "\n",
    "                    sampledSet = poolData.loc[poolData[iID].isin(sampledIndices)]\n",
    "                    training_data.reset_index(drop=True)\n",
    "                    sampledSet.reset_index(drop=True)\n",
    "                    training_data = pd.concat([training_data, sampledSet], axis=0).reset_index(drop=True)\n",
    "                    training_data = training_data.sort_values(iID)\n",
    "                    indices = unlabeled_data.loc[unlabeled_data[iID].isin(sampledIndices)].index.to_list()\n",
    "                    unlabeled_data = unlabeled_data.drop(indices).reset_index(drop=True)\n",
    "                    unlabeled_data = unlabeled_data.reset_index(drop=True)\n",
    "  \n",
    "                    train_data = pool.poolDataEmb[poolData.index[poolData[iID].isin(training_data[iID].values)].tolist()]\n",
    "                    train_labels = np.array(training_data[goldLabel].tolist())\n",
    "\n",
    "                    model.fit(train_data, train_labels) \n",
    "                    evaluate(train_data, train_labels, poolDataEmb_val, validation_data, poolDataEmb_test, test_data, mClass, sampledIndices, log_path, alBatch + 1)     "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ALforCalibration_MLP_withCostF_simplified.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}