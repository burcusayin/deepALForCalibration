{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ALforCalibration_MLP_withCostF_simplified.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "en5IRda4XIVp"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install sklearn\n",
        "!pip install netcal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwU1cqCKXmSi"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "from random import shuffle\n",
        "\n",
        "from netcal.metrics import ECE\n",
        "from sklearn.metrics import precision_recall_fscore_support, brier_score_loss, confusion_matrix\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPfLOR2CKB6Z"
      },
      "source": [
        "# please delete this cell if you don't run on Colab\n",
        "## Mount Drive into Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQmf-qpKpICj"
      },
      "source": [
        "#please define all the configurations for your dataset in this cell, and then run the code\n",
        "#configurations \n",
        "res_path                = 'drive/My Drive/Colab Notebooks/deepALForCalibration/res/withCost/'                            # specify the path to keep results\n",
        "data_folder             = 'drive/My Drive/Colab Notebooks/deepALForCalibration/datasets/binary/disaster_relevance/'  #specify the path to the folder where you keep your datasets\n",
        "dataToTrain             = '2_train_indexed_disaster_relevance_binary.csv'    # file name for your training data\n",
        "dataToVal               = '2_val_indexed_disaster_relevance_binary.csv'      # file name for your validation data\n",
        "dataToTest              = '2_test_indexed_disaster_relevance_binary.csv'     # file name for your test data\n",
        "logfile_name            = \"2-disasterRelevance-MLP3\"                         # specify the name of the result file\n",
        "#al_strategy             = 'random'                                          # specify the active learning strategy you want to use; 'random', 'diversity', or 'uncertainty'\n",
        "al_strategies           = ['random', 'uncertainty', 'diversity']\n",
        "minimum_training_items  = 157                                                # minimum number of training items before we first train a model\n",
        "alBatchNum              = 10                                                 # define the total number of batches in active learning pipeline\n",
        "alBatchSize             = 740                                                # define the size of one batch in active learning pipeline\n",
        "maxTfIdfFeat            = 1024                                               # define the maximum number of features for tfidf \n",
        "cfpList                     = [1]                                            # define the cost of obtaining a false positive\n",
        "cfnList                     = [1, 10, 100, 1000]                             # define the cost of obtaining a false negative\n",
        "chList                      = [1, 10, 100, 1000]                             # cost of asking humans\n",
        "\n",
        "# columns of the csv file used in the experiments: text/content for each item, gold labels for each item, confidence scores for each class, ID of each item \n",
        "# specify the column names of your data\n",
        "iID                     = 'itemID'                                          # give each item an ID, it will be used during active learning\n",
        "goldLabel               = 'crowd_label'                                     # define the name of column where you keep the gold labels of your data\n",
        "txt                     = 'text'                                            # define the name of column where you keep the items \n",
        "testGoldLabel           = 'gold_label'                                      # define the name of column where you keep the gold labels of your test data\n",
        "\n",
        "model = MLPClassifier(hidden_layer_sizes=(100, 100, 100, 100, 100), max_iter=500, alpha=0.05, activation = 'tanh', solver='sgd')  # define the model parameters that best fit to your data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH4SinJeXRWo"
      },
      "source": [
        "poolDataEmb_train = np.array([])\n",
        "poolDataEmb_val = np.array([])\n",
        "poolDataEmb_test = np.array([])\n",
        "\n",
        "# data directories\n",
        "unlabeled_data_dir = data_folder + dataToTrain\n",
        "validation_data_dir = data_folder + dataToVal\n",
        "test_data_dir = data_folder + dataToTest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVYyaN0_Z5Fd"
      },
      "source": [
        "class DiversitySampling():\n",
        "\n",
        "    def __init__(self, verbose):\n",
        "        self.verbose = verbose\n",
        "    \n",
        "    def get_validation_rankings(self, model, validation_data, val_emb):\n",
        "        \"\"\"Get model outliers from unlabeled data \n",
        "    \n",
        "        Keyword arguments:\n",
        "            model -- current Machine Learning model for this task\n",
        "            unlabeled_data -- data that does not yet have a label\n",
        "            validation_data -- held out data drawn from the same distribution as the training data\n",
        "            number -- number of items to sample\n",
        "            limit -- sample from only this many items for faster sampling (-1 = no limit)\n",
        "    \n",
        "        An outlier is defined as \n",
        "        unlabeled_data with the lowest average from rank order of logits\n",
        "        where rank order is defined by validation data inference \n",
        "    \n",
        "        \"\"\"\n",
        "                \n",
        "        validation_rankings = [] # 2D array, every neuron by ordered list of output on validation data per neuron    \n",
        "    \n",
        "        # Get per-neuron scores from validation data\n",
        "        if self.verbose:\n",
        "            print(\"Getting neuron activation scores from validation data\")\n",
        "\n",
        "        pred = model.predict_proba(val_emb) \n",
        "\n",
        "        v = 0\n",
        "        for neuron_outputs in pred:\n",
        "            # initialize array if we haven't yet\n",
        "            if len(validation_rankings) == 0:\n",
        "                for output in list(neuron_outputs):\n",
        "                    validation_rankings.append([0.0] * len(validation_data))\n",
        "\n",
        "            n=0\n",
        "            for output in list(neuron_outputs):\n",
        "                validation_rankings[n][v] = output\n",
        "                n += 1\n",
        "            v +=1\n",
        "            \n",
        "        \n",
        "        # Rank-order the validation scores \n",
        "        v=0\n",
        "        for validation in validation_rankings:\n",
        "            validation.sort() \n",
        "            validation_rankings[v] = validation\n",
        "            v += 1\n",
        "          \n",
        "        return validation_rankings \n",
        "    \n",
        "    def get_rank(self, value, rankings):\n",
        "        \"\"\" get the rank of the value in an ordered array as a percentage \n",
        "    \n",
        "        Keyword arguments:\n",
        "            value -- the value for which we want to return the ranked value\n",
        "            rankings -- the ordered array in which to determine the value's ranking\n",
        "        \n",
        "        returns linear distance between the indexes where value occurs, in the\n",
        "        case that there is not an exact match with the ranked values    \n",
        "        \"\"\"\n",
        "        \n",
        "        index = 0 # default: ranking = 0\n",
        "        \n",
        "        for ranked_number in rankings:\n",
        "            if value < ranked_number:\n",
        "                break #NB: this O(N) loop could be optimized to O(log(N))\n",
        "            index += 1        \n",
        "        \n",
        "        if(index >= len(rankings)):\n",
        "            index = len(rankings) # maximum: ranking = 1\n",
        "            \n",
        "        elif(index > 0):\n",
        "            # get linear interpolation between the two closest indexes \n",
        "            \n",
        "            diff = rankings[index] - rankings[index - 1]\n",
        "            perc = value - rankings[index - 1]\n",
        "            linear = perc / diff\n",
        "            index = float(index - 1) + linear\n",
        "        \n",
        "        absolute_ranking = index / len(rankings)\n",
        "    \n",
        "        return(absolute_ranking)\n",
        "    \n",
        "    def get_model_outliers(self, dataPool, model, unlabeled_data, unl_emb, validation_data, val_emb, number):\n",
        "        \"\"\"Get model outliers from unlabeled data \n",
        "    \n",
        "        Keyword arguments:\n",
        "            model -- current Machine Learning model for this task\n",
        "            unlabeled_data -- data that does not yet have a label\n",
        "            validation_data -- held out data drawn from the same distribution as the training data\n",
        "            number -- number of items to sample\n",
        "            limit -- sample from only this many items for faster sampling (-1 = no limit)\n",
        "    \n",
        "        An outlier is defined as \n",
        "        unlabeled_data with the lowest average from rank order of logits\n",
        "        where rank order is defined by validation data inference \n",
        "    \n",
        "        \"\"\"\n",
        "    \n",
        "        # Get per-neuron scores from validation data\n",
        "        validation_rankings = self.get_validation_rankings(model, validation_data, val_emb)\n",
        "\n",
        "        # Iterate over unlabeled items\n",
        "        if self.verbose:\n",
        "            print(\"Getting rankings for unlabeled data\")\n",
        "    \n",
        "        outliers = []\n",
        "        pred = model.predict_proba(unl_emb) \n",
        "\n",
        "        itID = 0\n",
        "        for neuron_outputs in pred:\n",
        "            n=0\n",
        "            ranks = []\n",
        "            for output in neuron_outputs:\n",
        "                rank = self.get_rank(output, validation_rankings[n])\n",
        "                ranks.append(rank)\n",
        "                n += 1 \n",
        "            avgRank = 1 - (sum(ranks) / len(neuron_outputs)) # average rank\n",
        "            currentRow = unlabeled_data.iloc[[itID]].reset_index(drop=True)\n",
        "            rowIndex = currentRow.itemID.item()\n",
        "            row = dataPool.loc[dataPool[iID] == rowIndex]\n",
        "            row['avgRank'] = avgRank\n",
        "            outliers.append(row.values.flatten().tolist()) \n",
        "            itID += 1\n",
        "        outliers.sort(reverse=True, key=lambda x: x[-1])       \n",
        "        return outliers[:number:]       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-pASfloTKzi"
      },
      "source": [
        "def random_sampling(dataIds, nQuery):\n",
        "    '''Randomly samples the points'''\n",
        "    query_idx = random.sample(range(len(dataIds)), nQuery)\n",
        "    selectedIndex = dataIds[query_idx]\n",
        "    return selectedIndex\n",
        "        \n",
        "def uncertainty_sampling(model, unl_emb, number):\n",
        "    '''Points are sampled according to uncertainty sampling criterion'''\n",
        "\n",
        "    pred = model.predict_proba(unl_emb)\n",
        "    uncertainty_scores = 1 - pred.max(axis=1)\n",
        "    score_indices = np.argsort(uncertainty_scores)\n",
        "    return score_indices[-number:]   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4EwXlvgazxW"
      },
      "source": [
        "## Feature Preparation\n",
        "def prepare_features(X_train, min_df=2, max_features=None, ngram_range=(1, 3)):\n",
        "    # compute tfidf features\n",
        "    tfidf = TfidfVectorizer(min_df=min_df, max_features=max_features,\n",
        "                strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n",
        "                ngram_range=ngram_range, use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
        "                stop_words=None, lowercase=False)\n",
        "\n",
        "    X_train_tfidf = tfidf.fit_transform(X_train).toarray()\n",
        "    return X_train_tfidf\n",
        "\n",
        "class Data():\n",
        "    \n",
        "    def __init__(self, filename):\n",
        "        \n",
        "        # each dataset will have a pool of data, together with their IDs and gold labels \n",
        "        self.poolData = np.array([])\n",
        "        self.poolGoldLabels = np.array([])\n",
        "        \n",
        "        dt = pd.read_csv(filename)\n",
        "        dt = dt.dropna()\n",
        "        dt = dt.reset_index(drop=True)\n",
        "        y = dt[goldLabel].values\n",
        "        X = prepare_features(dt[txt].tolist(), min_df= 0, max_features = maxTfIdfFeat, ngram_range = (1, 3))\n",
        "        self.data = dt\n",
        "        self.poolDataEmb = X\n",
        "        self.poolGoldLabels = y\n",
        "        self.mClass = list(set(self.poolGoldLabels.tolist()))\n",
        "        \n",
        "    def setStartState(self, nStart):\n",
        "        ''' This functions creates the initial training set which contains the equal number of samples per class\n",
        "        Input:\n",
        "        nStart -- number of labelled datapoints (size of training set)\n",
        "        '''\n",
        "        self.nStart = nStart\n",
        "        data = self.data.copy()\n",
        "        # get predefined points so that all classes are represented and initial classifier could be trained.\n",
        "        sampledIndices = []\n",
        "        for cls in self.mClass:\n",
        "            indices = np.array(np.where(self.poolGoldLabels == cls)).tolist()[0]\n",
        "            idx = random.sample(indices, nStart // len(mClass))\n",
        "            sampledIndices = sampledIndices + idx\n",
        "\n",
        "        sData = data.iloc[sampledIndices]\n",
        "        self.labeledSet = sData.reset_index(drop=True)\n",
        "        droppedData = data.drop(sampledIndices)\n",
        "        self.unlabeledSet = droppedData.reset_index(drop=True)\n",
        "\n",
        "# function to calculate the ECE score\n",
        "def ece_score(y_true, y_prob, n_bins=10):\n",
        "    ece = ECE(n_bins)\n",
        "    ece_val = ece.measure(y_prob, y_true)\n",
        "\n",
        "    return ece_val\n",
        "\n",
        "# classify by the threshold with respect to cost of different errors\n",
        "def classify(probs, goldLabels):\n",
        "    y_clf = []\n",
        "    y_pred = []\n",
        "    for i in range(len(probs)):\n",
        "        p = probs[i][1]  #probability of being positive\n",
        "        if ((cfp*(1-p)) < (cfn*p)) and ((cfp*(1-p)) < ch):\n",
        "            y_pred.append(1)\n",
        "            y_clf.append(1)\n",
        "        elif ((cfn*p) < (cfp*(1-p))) and ((cfn*p) < ch):\n",
        "            y_pred.append(0)\n",
        "            y_clf.append(0)\n",
        "        else:\n",
        "            y_clf.append(-1)\n",
        "            y_pred.append(goldLabels[i])\n",
        "    return y_clf, np.array(y_pred)\n",
        "\n",
        "#calculate the total cost of predictions\n",
        "def calculateCost(fp, fn, y_clf):\n",
        "    uc = y_clf.count(-1)\n",
        "    cost = (cfp*fp + cfn*fn + ch*uc) / len(y_clf)\n",
        "    return cost, uc\n",
        "\n",
        "#evaluate the trained model on test/validation/training sets\n",
        "def evaluate(train_data, train_labels, poolDataEmb_val, validation_data, poolDataEmb_test, test_data, mClass, sampledIndices, res_path, alBatch):\n",
        "\n",
        "    logits_train = model.predict_proba(train_data)\n",
        "    probs_train = np.array(logits_train)\n",
        "    y_clf_train, y_pred_train = classify(probs_train, train_labels)\n",
        "\n",
        "    logits_val = model.predict_proba(poolDataEmb_val)\n",
        "    probs_val = np.array(logits_val)\n",
        "    val_labels = np.array(validation_data[goldLabel].tolist())\n",
        "    y_clf_val, y_pred_val = classify(probs_val, val_labels)\n",
        "\n",
        "    logits_test = model.predict_proba(poolDataEmb_test)\n",
        "    probs_test = np.array(logits_test)\n",
        "    test_labels = np.array(test_data[testGoldLabel].tolist())\n",
        "    y_clf_test, y_pred_test = classify(probs_test, test_labels)\n",
        "\n",
        "    # check if binary or multi class classification\n",
        "    if len(mClass) == 2:\n",
        "        average = 'binary'\n",
        "    else:\n",
        "        average = 'macro'\n",
        "\n",
        "    sampledItems = ''.join(str(e)+' ' for e in sampledIndices)\n",
        "\n",
        "    pre_train, rec_train, f1_train, _ = precision_recall_fscore_support(train_labels, y_pred_train, average=average, beta=1)\n",
        "    ece_train = ece_score(train_labels, probs_train)\n",
        "    _, _, f01_train, _ = precision_recall_fscore_support(train_labels, y_pred_train, average=average, beta=0.1)\n",
        "    _, _, f10_train, _ = precision_recall_fscore_support(train_labels, y_pred_train, average=average, beta=10)\n",
        "\n",
        "    pre_val, rec_val, f1_val, _ = precision_recall_fscore_support(val_labels, y_pred_val, average=average, beta=1)\n",
        "    ece_val = ece_score(val_labels, probs_val)\n",
        "    _, _, f01_val, _ = precision_recall_fscore_support(val_labels, y_pred_val, average=average, beta=0.1)\n",
        "    _, _, f10_val, _ = precision_recall_fscore_support(val_labels, y_pred_val, average=average, beta=10)\n",
        "\n",
        "    pre_test, rec_test, f1_test, _ = precision_recall_fscore_support(test_labels, y_pred_test, average=average, beta=1)\n",
        "    ece_test = ece_score(test_labels, probs_test)\n",
        "    _, _, f01_test, _ = precision_recall_fscore_support(test_labels, y_pred_test, average=average, beta=0.1)\n",
        "    _, _, f10_test, _ = precision_recall_fscore_support(test_labels, y_pred_test, average=average, beta=10)\n",
        "\n",
        "    if average == 'binary':\n",
        "        brier_train = brier_score_loss(train_labels, probs_train[:,1])\n",
        "        brier_val = brier_score_loss(val_labels, probs_val[:,1])\n",
        "        brier_test = brier_score_loss(test_labels, probs_test[:,1])\n",
        "\n",
        "        tn_train, fp_train, fn_train, tp_train = confusion_matrix(train_labels, y_pred_train).ravel()\n",
        "        tn_val, fp_val, fn_val, tp_val = confusion_matrix(val_labels, y_pred_val).ravel()\n",
        "        tn_test, fp_test, fn_test, tp_test = confusion_matrix(test_labels, y_pred_test).ravel()\n",
        "\n",
        "        cost_train, uc_train = calculateCost(fp_train, fn_train, y_clf_train)\n",
        "        cost_val, uc_val = calculateCost(fp_val, fn_val, y_clf_val)\n",
        "        cost_test, uc_test = calculateCost(fp_test, fn_test, y_clf_test)\n",
        "        \n",
        "        print(\n",
        "            'Iteration: {}. F1: {:1.3f}, Precision: {:1.3f}, Recall: {:1.3f}'.\n",
        "            format(alBatch, f1_val, pre_val, rec_val))\n",
        "        # print to result file\n",
        "        with open(res_path, 'a') as f:\n",
        "            res_i = '{}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}\\n'.format(alBatch, sampledItems, pre_train, rec_train, f01_train, f1_train, f10_train, ece_train, brier_train, cost_train, uc_train, pre_val, rec_val, f01_val, f1_val, f10_val, ece_val, brier_val, cost_val, uc_val, pre_test, rec_test, f01_test, f1_test, f10_test, ece_test, brier_test, cost_test, uc_test)\n",
        "            f.write(res_i)\n",
        "    else:\n",
        "\n",
        "        print(\n",
        "            'Iteration: {}. F1: {:1.3f}, Precision: {:1.3f}, Recall: {:1.3f}'.\n",
        "            format(alBatch, f1_val, pre_val, rec_val))\n",
        "        # print to result file\n",
        "        with open(res_path, 'a') as f:\n",
        "            res_i = '{}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}\\n'.format(alBatch, sampledItems, pre_train, rec_train, f01_train, f1_train, f10_train, ece_train, pre_val, rec_val, f01_val, f1_val, f10_val, ece_val, pre_test, rec_test, f01_test, f1_test, f10_test, ece_test)\n",
        "            f.write(res_i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieL71IeGa1VT"
      },
      "source": [
        "#load datasets\n",
        "pool = Data(unlabeled_data_dir)\n",
        "mClass =  pool.mClass\n",
        "pool.setStartState(minimum_training_items)\n",
        "\n",
        "validation = Data(validation_data_dir)\n",
        "validation_data = validation.data\n",
        "test = Data(test_data_dir)\n",
        "test_data = test.data\n",
        "\n",
        "for al_strategy in al_strategies:\n",
        "    for cfp in cfpList:\n",
        "        for cfn in cfnList:\n",
        "            for ch in chList:\n",
        "                poolData = pool.data\n",
        "                training_data = pool.labeledSet\n",
        "                unlabeled_data = pool.unlabeledSet\n",
        "\n",
        "                poolDataEmb_val = validation.poolDataEmb\n",
        "                poolDataEmb_test = test.poolDataEmb\n",
        "\n",
        "                train_data = pool.poolDataEmb[poolData.index[poolData[iID].isin(training_data[iID].values)].tolist()]\n",
        "                train_labels = np.array(training_data[goldLabel].tolist())\n",
        "\n",
        "                #Start active learning\n",
        "                sampleIds = []\n",
        "                samplingRanks = []\n",
        "\n",
        "                log_name = al_strategy + '_' + logfile_name + \"_cfp_{}_cfn_{}_ch_{}.csv\".format(cfp, cfn, ch)\n",
        "\n",
        "                # create log file\n",
        "                log_path = res_path + log_name\n",
        "                if len(mClass) == 2:\n",
        "                    with open(log_path, 'w') as f:\n",
        "                        c = 'alBatch, sampledIndices, pre_train, rec_train, f01_train, f1_train, f10_train, ece_train, brier_train, cost_train, uc_train, pre_val, rec_val, f01_val, f1_val, f10_val, ece_val, brier_val, cost_val, uc_val, pre_test, rec_test, f01_test, f1_test, f10_test, ece_test, brier_test, cost_test, uc_test'\n",
        "                        f.write(c + '\\n')\n",
        "                else:\n",
        "                    with open(log_path, 'w') as f:\n",
        "                        c = 'alBatch, sampledIndices, pre_train, rec_train, f01_train, f1_train, f10_train, ece_train, pre_val, rec_val, f01_val, f1_val, f10_val, ece_val, pre_test, rec_test, f01_test, f1_test, f10_test, ece_test'\n",
        "                        f.write(c + '\\n')\n",
        "\n",
        "                model.fit(train_data, train_labels) \n",
        "                evaluate(train_data, train_labels, poolDataEmb_val, validation_data, poolDataEmb_test, test_data, mClass, [], log_path, 0)\n",
        "\n",
        "                for alBatch in range(alBatchNum):\n",
        "                    sampledIndices = []\n",
        "\n",
        "                    unl_dataEmb = pool.poolDataEmb[poolData.index[poolData[iID].isin(unlabeled_data[iID].values)].tolist()]\n",
        "\n",
        "                    if al_strategy == 'diversity':\n",
        "                        strategy = DiversitySampling(True)\n",
        "                        sampledItems = strategy.get_model_outliers(poolData, model, unlabeled_data, unl_dataEmb, validation_data, poolDataEmb_val, number=alBatchSize) \n",
        "        \n",
        "                        for outlier in sampledItems:\n",
        "                            samplingRanks.append(outlier[-1])\n",
        "                            sampleIds.append(outlier[-2])\n",
        "                            sampledIndices.append(outlier[-2])\n",
        "   \n",
        "                    elif al_strategy == 'random':\n",
        "                        sampledIndices = random_sampling(unlabeled_data[iID].values, alBatchSize)\n",
        "                        for i in sampledIndices: sampleIds.append(i)\n",
        "                    elif al_strategy == 'uncertainty':\n",
        "                        idx = uncertainty_sampling(model, unl_dataEmb, alBatchSize)\n",
        "                        sampledIndices = unlabeled_data.loc[idx][iID].tolist()\n",
        "                        for i in sampledIndices: sampleIds.append(i)\n",
        "                    else:\n",
        "                        # random sampling by default\n",
        "                        sampledIndices = random_sampling(unlabeled_data[iID].values, alBatchSize)\n",
        "                        for i in sampledIndices: sampleIds.append(i)\n",
        "\n",
        "                    sampledSet = poolData.loc[poolData[iID].isin(sampledIndices)]\n",
        "                    training_data.reset_index(drop=True)\n",
        "                    sampledSet.reset_index(drop=True)\n",
        "                    training_data = pd.concat([training_data, sampledSet], axis=0).reset_index(drop=True)\n",
        "                    training_data = training_data.sort_values(iID)\n",
        "                    indices = unlabeled_data.loc[unlabeled_data[iID].isin(sampledIndices)].index.to_list()\n",
        "                    unlabeled_data = unlabeled_data.drop(indices).reset_index(drop=True)\n",
        "                    unlabeled_data = unlabeled_data.reset_index(drop=True)\n",
        "  \n",
        "                    train_data = pool.poolDataEmb[poolData.index[poolData[iID].isin(training_data[iID].values)].tolist()]\n",
        "                    train_labels = np.array(training_data[goldLabel].tolist())\n",
        "\n",
        "                    model.fit(train_data, train_labels) \n",
        "                    evaluate(train_data, train_labels, poolDataEmb_val, validation_data, poolDataEmb_test, test_data, mClass, sampledIndices, log_path, alBatch + 1)     "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}