{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SearchHyperParams.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLY21Md8Olfw"
      },
      "source": [
        "!pip install sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xcnJAaBO3pn"
      },
      "source": [
        "## Mount Drive into Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-HKC2N4P5g6"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QoLRCtrO9A5"
      },
      "source": [
        "data_folder = 'drive/My Drive/Colab Notebooks/deepALForCalibration/datasets/binary/chemicals_disease/' #specify the path to the folder where you keep your datasets\n",
        "dataToTrain = '3_train_indexed_chemicals_disease_binary.csv'              # file name for your training data\n",
        "dataToVal = '3_val_indexed_chemicals_disease_binary.csv'                  # file name for your validation data\n",
        "dataToTest = '3_test_indexed_chemicals_disease_binary.csv'                # file name for your test data\n",
        "\n",
        "# columns of the csv file used in the experiments: text/content for each item, gold labels for each item, confidence scores for each class, ID of each item \n",
        "# specify the column names of your data\n",
        "iID = 'itemID'             # give each item an ID, it will be used during active learning\n",
        "goldLabel = 'crowd_label'  # define the name of column where you keep the gold labels of your data\n",
        "txt = 'text'               # define the name of column where you keep the items \n",
        "\n",
        "parameter_space = {                                                   # define the parameter space you want to search on\n",
        "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
        "    'activation': ['tanh', 'relu'],\n",
        "    'solver': ['sgd', 'adam'],\n",
        "    'alpha': [0.0001, 0.05],\n",
        "    'learning_rate': ['constant','adaptive'],\n",
        "}\n",
        "\n",
        "maxFeat = 1024            # define the maximum number of features you want to obtain in tfidf vectors\n",
        "nGramRange  = (1, 3)      # define the ngram range\n",
        "\n",
        "\n",
        "# specify data directories\n",
        "unlabeled_data_dir = data_folder + dataToTrain\n",
        "validation_data_dir = data_folder + dataToVal\n",
        "test_data_dir = data_folder + dataToTest\n",
        "\n",
        "# PARAMETERS\n",
        "num_labels = 2                                                       # number of classes in your data\n",
        "mClass = [0, 1]                                                    # define all of possible classes\n",
        "minimum_training_items = 86                                           # minimum number of training items before we first train a model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbhWrzNWPD6O"
      },
      "source": [
        "## Feature Preparation\n",
        "def prepare_features(X_train, min_df=2, max_features=None, ngram_range=(1, 3)):\n",
        "    # compute tfidf features\n",
        "    tfidf = TfidfVectorizer(min_df=min_df, max_features=max_features,\n",
        "                strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n",
        "                ngram_range=ngram_range, use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
        "                stop_words=None, lowercase=False)\n",
        "\n",
        "    X_train_tfidf = tfidf.fit_transform(X_train).toarray()\n",
        "    print(\"X_train_tfidf.shape\", X_train_tfidf.shape)\n",
        "    return X_train_tfidf\n",
        "\n",
        "class Data():\n",
        "    \n",
        "    def __init__(self, filename):\n",
        "        \n",
        "        # each dataset will have a pool of data, together with their IDs and gold labels \n",
        "        self.poolData = np.array([])\n",
        "        self.poolGoldLabels = np.array([])\n",
        "        \n",
        "        dt = pd.read_csv(filename)\n",
        "        indices = dt[iID].values\n",
        "        y = dt[goldLabel].values\n",
        "        X = prepare_features(dt[txt].tolist(), min_df= 0, max_features = maxFeat, ngram_range = nGramRange)\n",
        "        \n",
        "        self.data = dt\n",
        "        self.poolDataEmb = X\n",
        "        self.poolGoldLabels = y\n",
        "        self.poolDataIndices = indices\n",
        "        \n",
        "    def setStartState(self, nStart):\n",
        "        ''' This functions initialises fields indicesKnown and indicesUnknown which contain the datapoints having final labels(known) and still explorable(unknown) ones.\n",
        "        Input:\n",
        "        nStart -- number of labelled datapoints (size of indicesKnown)\n",
        "        '''\n",
        "        self.nStart = nStart\n",
        "        self.indicesKnown = np.array([])\n",
        "        self.indicesUnknown = np.array([])\n",
        "        \n",
        "        # get predefined points so that all classes are represented and initial classifier could be trained.\n",
        "\n",
        "        for cls in mClass:\n",
        "            indices = np.array(np.where(self.poolGoldLabels == cls)).tolist()[0]\n",
        "            sampledIndices = random.sample(indices, nStart // len(mClass))\n",
        "            dataIndices = np.array(self.poolDataIndices)\n",
        "            if self.indicesKnown.size == 0:\n",
        "                self.indicesKnown = dataIndices[sampledIndices]\n",
        "            else:\n",
        "                self.indicesKnown = np.concatenate(([self.indicesKnown, dataIndices[sampledIndices]])); \n",
        "        for i in self.poolDataIndices:\n",
        "            if i not in self.indicesKnown:\n",
        "                if self.indicesUnknown.size == 0:\n",
        "                    self.indicesUnknown = np.array([i])\n",
        "                else:\n",
        "                    self.indicesUnknown = np.concatenate(([self.indicesUnknown, np.array([i])]));\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oC4f7bLHPMay"
      },
      "source": [
        "pool = Data(unlabeled_data_dir)\n",
        "pool.setStartState(minimum_training_items)\n",
        "poolData = pool.data\n",
        "poolDataIndices = pool.poolDataIndices\n",
        "poolDataEmb_train = pool.poolDataEmb\n",
        "\n",
        "train_labels = np.array(poolData[goldLabel].tolist())\n",
        "\n",
        "model = GridSearchCV(MLPClassifier(max_iter=500), parameter_space, n_jobs=-1, cv=3)\n",
        "  \n",
        "model.fit(poolDataEmb_train, train_labels) \n",
        "\n",
        "# Best paramete set\n",
        "print('Best parameters found:\\n', model.best_params_)\n",
        "\n",
        "# All results\n",
        "means = model.cv_results_['mean_test_score']\n",
        "stds = model.cv_results_['std_test_score']\n",
        "for mean, std, params in zip(means, stds, model.cv_results_['params']):\n",
        "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}